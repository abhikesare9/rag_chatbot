2025-02-03 13:56:53,268 - INFO - db configuration is localhost, 8000
2025-02-03 13:56:53,413 - INFO - successfully connected to chromadb
2025-02-03 14:01:01,261 - INFO - db configuration is localhost, 8000
2025-02-03 14:01:01,413 - INFO - successfully connected to chromadb
2025-02-03 14:34:21,913 - INFO - db configuration is localhost, 8000
2025-02-03 14:34:22,056 - INFO - successfully connected to chromadb
2025-02-03 14:34:47,554 - INFO - db configuration is localhost, 8000
2025-02-03 14:34:47,725 - INFO - successfully connected to chromadb
2025-02-03 14:35:31,889 - INFO - db configuration is localhost, 8000
2025-02-03 14:35:32,063 - INFO - successfully connected to chromadb
2025-02-03 14:35:43,873 - INFO - db configuration is localhost, 8000
2025-02-03 14:35:44,012 - INFO - successfully connected to chromadb
2025-02-03 14:38:14,166 - INFO - db configuration is localhost, 8000
2025-02-03 14:38:14,318 - INFO - successfully connected to chromadb
2025-02-03 14:38:23,460 - INFO - db configuration is localhost, 8000
2025-02-03 14:38:23,610 - INFO - successfully connected to chromadb
2025-02-03 14:38:40,013 - INFO - db configuration is localhost, 8000
2025-02-03 14:38:40,154 - INFO - successfully connected to chromadb
2025-02-03 14:41:05,224 - INFO - db configuration is localhost, 8000
2025-02-03 14:41:05,364 - INFO - successfully connected to chromadb
2025-02-03 14:41:10,617 - INFO - Invalid query
2025-02-03 14:45:39,744 - INFO - db configuration is localhost, 8000
2025-02-03 14:45:39,907 - INFO - successfully connected to chromadb
2025-02-03 14:46:18,187 - INFO - Text extracted from pdf and converted into chunks
2025-02-03 14:46:26,043 - ERROR - Failed to get relevent passage, Please change the query
2025-02-03 14:46:26,044 - ERROR - Failed to perform db operations 'ChromaDBSingleton' object has no attribute 'query'
2025-02-05 13:47:20,311 - INFO - db configuration is localhost, 8000
2025-02-05 13:47:20,539 - INFO - successfully connected to chromadb
2025-02-05 13:48:10,386 - INFO - db configuration is localhost, 8000
2025-02-05 13:48:10,584 - INFO - successfully connected to chromadb
2025-02-05 13:50:36,892 - INFO - Text extracted from pdf and converted into chunks
2025-02-05 13:50:36,899 - ERROR - Failed to perform db operations Collection Microservices-Design-Patterns.pdf already exists
2025-02-05 13:53:50,084 - INFO - db configuration is localhost, 8000
2025-02-05 13:53:50,227 - INFO - successfully connected to chromadb
2025-02-05 13:53:56,835 - INFO - Text extracted from pdf and converted into chunks
2025-02-05 13:53:56,841 - ERROR - Failed to perform db operations Collection Microservices-Design-Patterns.pdf already exists
2025-02-05 13:56:23,837 - INFO - db configuration is localhost, 8000
2025-02-05 13:56:24,008 - INFO - successfully connected to chromadb
2025-02-05 13:56:32,194 - INFO - Text extracted from pdf and converted into chunks
2025-02-05 13:56:32,195 - ERROR - Failed to perform db operations can only concatenate str (not "UUID") to str
2025-02-05 13:56:59,808 - INFO - db configuration is localhost, 8000
2025-02-05 13:56:59,988 - INFO - successfully connected to chromadb
2025-02-05 13:57:08,002 - INFO - Text extracted from pdf and converted into chunks
2025-02-05 13:57:08,010 - ERROR - Failed to perform db operations Expected collection name that (1) contains 3-63 characters, (2) starts and ends with an alphanumeric character, (3) otherwise contains only alphanumeric characters, underscores or hyphens (-), (4) contains no two consecutive periods (..) and (5) is not a valid IPv4 address, got Microservices-Design-Patterns.pdf0bd380f2-d404-4cfc-9301-04688452dcf0
2025-02-05 13:59:53,507 - INFO - db configuration is localhost, 8000
2025-02-05 13:59:53,666 - INFO - successfully connected to chromadb
2025-02-05 14:00:01,136 - INFO - Text extracted from pdf and converted into chunks
2025-02-05 14:00:08,028 - INFO - (Collection(name=Microservices-Design-Patterns.pdf), 'Microservices-Design-Patterns.pdf')
2025-02-05 14:00:08,039 - ERROR - Failed to get relevent passage, Please change the query
2025-02-05 14:00:08,040 - ERROR - Failed to perform db operations 'ChromaDBSingleton' object has no attribute 'query'
2025-02-05 14:01:54,441 - INFO - Text extracted from pdf and converted into chunks
2025-02-05 14:02:00,895 - INFO - (Collection(name=Microservices-Design-Patterns.pdf), 'Microservices-Design-Patterns.pdf')
2025-02-05 14:02:00,903 - ERROR - Failed to get relevent passage, Please change the query
2025-02-05 14:02:00,904 - ERROR - Failed to perform db operations 'ChromaDBSingleton' object has no attribute 'query'
2025-02-05 14:04:37,140 - INFO - db configuration is localhost, 8000
2025-02-05 14:04:37,280 - INFO - successfully connected to chromadb
2025-02-05 14:04:43,314 - INFO - Text extracted from pdf and converted into chunks
2025-02-05 14:04:51,138 - INFO - (Collection(name=Microservices-Design-Patterns.pdf), 'Microservices-Design-Patterns.pdf')
2025-02-05 14:04:51,147 - ERROR - Failed to get relevent passage, Please change the query
2025-02-05 14:04:51,147 - ERROR - Failed to perform db operations get_relevant_passage() missing 1 required positional argument: 'collection_name'
2025-02-05 14:05:21,860 - INFO - db configuration is localhost, 8000
2025-02-05 14:05:22,025 - INFO - successfully connected to chromadb
2025-02-05 14:05:33,601 - INFO - Text extracted from pdf and converted into chunks
2025-02-05 14:05:40,361 - INFO - (Collection(name=Microservices-Design-Patterns.pdf), 'Microservices-Design-Patterns.pdf')
2025-02-05 14:05:40,374 - ERROR - Failed to get relevent passage, Please change the query
2025-02-05 14:05:40,374 - ERROR - Failed to perform db operations 'ChromaDBSingleton' object has no attribute 'collection_name'
2025-02-05 14:06:45,132 - INFO - db configuration is localhost, 8000
2025-02-05 14:06:45,350 - INFO - successfully connected to chromadb
2025-02-05 14:06:53,092 - INFO - Text extracted from pdf and converted into chunks
2025-02-05 14:06:57,576 - INFO - (Collection(name=Microservices-Design-Patterns.pdf), 'Microservices-Design-Patterns.pdf')
2025-02-05 14:06:57,589 - ERROR - Failed to get relevent passage, Please change the query
2025-02-05 14:06:57,589 - ERROR - Failed to perform db operations 'ChromaDBSingleton' object has no attribute 'collection_name'
2025-02-05 14:07:43,893 - INFO - db configuration is localhost, 8000
2025-02-05 14:07:44,048 - INFO - successfully connected to chromadb
2025-02-05 14:07:47,504 - INFO - Text extracted from pdf and converted into chunks
2025-02-05 14:07:56,622 - INFO - (Collection(name=Microservices-Design-Patterns.pdf), 'Microservices-Design-Patterns.pdf')
2025-02-05 14:07:56,631 - ERROR - Failed to get relevent passage, Please change the query
2025-02-05 14:07:56,631 - ERROR - Failed to perform db operations 'tuple' object has no attribute 'query'
2025-02-05 14:10:29,088 - INFO - db configuration is localhost, 8000
2025-02-05 14:10:29,232 - INFO - successfully connected to chromadb
2025-02-05 14:10:37,645 - INFO - Text extracted from pdf and converted into chunks
2025-02-05 14:10:37,645 - INFO - hello
2025-02-05 14:10:44,234 - INFO - (Collection(name=Microservices-Design-Patterns.pdf), 'Microservices-Design-Patterns.pdf')
2025-02-05 14:10:44,247 - ERROR - Failed to get relevent passage, Please change the query
2025-02-05 14:10:44,247 - ERROR - Failed to perform db operations 'tuple' object has no attribute 'query'
2025-02-05 14:12:51,455 - INFO - db configuration is localhost, 8000
2025-02-05 14:12:51,648 - INFO - successfully connected to chromadb
2025-02-05 14:12:56,457 - INFO -   
      
www.valuelabs.com 
Microservices  Design Patterns  Microservices Design Patterns    
 Copyright © 2023 ValueLabs. All rights reserved                                                                                                                   Page 1 of 17                                             
Table of Contents Introduction ........................................................................................................................................ 2 Abstract .............................................................................................................................................. 2 Introduction to Microservices .............................................................................................................. 2 Microservices Design Patterns and Principles ...................................................................................... 3 Database per Microservices ................................................................................................................ 5 Database changes can be made independently without impacting other Microservices: ............................... 5 CQRS Design Pattern ........................................................................................................................... 6 How to Sync Databases with CQRS? .............................................................................................................. 7 Event Sourcing .................................................................................................................................... 7 SAGA ................................................................................................................................................... 8 Choreography-based saga ............................................................................................................................. 9 Orchestration Based Saga ............................................................................................................................. 9 BFF (Backend for Frontend) ............................................................................................................... 10 Here is an example showing BFF: ................................................................................................................ 11 API Gateway ..................................................................................................................................... 12 Strangler Pattern .............................................................................................................................. 12 The Strangler Pattern follows these basic steps: .......................................................................................... 13 Circuit Breaker .................................................................................................................................. 13 Here are the basic steps involved in the Circuit Breaker Pattern: ................................................................. 14 Externalized Configuration ................................................................................................................ 14 Consumer-Driven Contract Tracing .................................................................................................... 15 Here are the steps to implement CDCT: ....................................................................................................... 15 Conclusion ......................................................................................................................................... 16         Microservices Design Patterns    
 Copyright © 2023 ValueLabs. All rights reserved                                                                                                                   Page 2 of 17                                             
  Introduction The purpose of this guide is to help you with architectural patterns followed while designing Microservices architecture.  Abstract This document aims to provide basic know-how of the ‘Microservices Design Patterns.’ We aim to provide a reference document for design patterns to be used when any application is designed using Microservices architecture.   We aim to publish this whitepaper as a ready reference to ten highly used design patterns related to Microservices in a detailed fashion.  Introduction to Microservices Microservices are self-contained and independent deployment modules. In Microservices, the application is divided into independent modules based on business domains. Microservices are designed based on Domain Driven Design (DDD), which says the application should be modeled around independent modules with bounded context for the specific business do-main. As we now have independent modules, we have faster rollout and quicker release cycles. All the modules can be worked on in parallel, and any changes will only affect that specific module. Therefore, Continuous Integration (CI) & Continu-ous Delivery (CD) are key advantages of using Microservices architecture.     As Monolithic applications pose their own set of challenges, here are four main concepts that describe the importance of Microservices architecture over Monolithic architecture. Some of the key points about Microservices are:  1. Loosely coupled components designed around business domains 2. Application can be distributed across different clouds and data centers 3. Change management is easy in Microservices 
In short, the Microservices architectural style is an approach to developing a single application as a suite of small services, each running in its own process and communicating with lightweight mechanisms, often an HTTP resource API. These services are built around business capabilities and are independently deployable by fully automated deployment machinery.   – Martin Fowler Microservices Design Patterns    
 Copyright © 2023 ValueLabs. All rights reserved                                                                                                                   Page 3 of 17                                             
  Microservices Design Patterns and Principles As we now know, Microservices are independent components that encapsulate business domains. If applications are archi-tected using Microservices design principles and patterns, then the application is highly scalable. So, let's list down the princi-ples in which the Microservices architecture has been built. 1. Scalability 2. Flexibility 3. Independent and autonomous 4. Decentralized governance 5. Resiliency 6. Failure isolation 7. Continuous delivery through the DevOps While adhering to the principles mentioned above, there are some standard sets of problems & complex scenarios that archi-tects and developers need to address. In addressing such common problems, statements bring proven solutions to the table, which become patterns. Here are a few high-level patterns that are used around Microservices.  
Microservices Design Patterns    
 Copyright © 2023 ValueLabs. All rights reserved                                                                                                                   Page 4 of 17                                             
                                         Picture Reference: https://microservices.io/patterns/microservices.html We will cover the most used and discussed design patterns in detail, along with use cases. Here is the list of design patterns that will be covered: 1. Database per Microservices 2. CQRS 3. Event Sourcing 4. Saga 5. BFF 6. API Gateway 7. Strangler 8. Circuit Breaker 9. Externalized Configuration 10. Consumer-Driven Contract Tracing    
Microservices Design Patterns    
 Copyright © 2023 ValueLabs. All rights reserved                                                                                                                   Page 5 of 17                                             
Database per Microservices    
 Microservices are generally independent and loosely bound components. So, to achieve this independent nature, every Microservice must have its own database so that it can be developed and deployed independently.   Let’s take the example of an e-commerce application. We will have product, ordering and SKU Microservices that each service interacts (store and retrieve) with data from their own databases. Any changes to one database don’t impact other Microservices.  Other Microservices can’t directly access each independent database. Each component’s persistent data can only be ac-cessed via APIs. So, Database per Microservices provides many benefits, especially to evolve rapidly and support massive scaling as they make each Microservice independent.   Database changes can be made independently without impacting other Microservices: • Each database can scale independently • Microservices domain data is encapsulated within the service • If one of the database servers is down, this will not affect other services Also *Polyglot data persistence gives the ability to select the best-optimized storage needs per Microservices. So, we can use this as an example - an e-commerce application with Microservices, as mentioned earlier. Here are the optimized choices: • The product Microservices use a NoSQL document database for storing catalog-related data, which is storing JSON objects to accommodate high volumes of read operations • The shopping cart Microservices use a distributed cache that supports its simple, key-value datastore • The ordering Microservices use a relational database to accommodate the rich relational structure of its underlying data 
Microservices Design Patterns    
 Copyright © 2023 ValueLabs. All rights reserved                                                                                                                   Page 6 of 17                                             
 Because of the ability to scale and high availability, NoSQL databases are getting highly popular and are widely used in enter-prise applications nowadays. Also, their support for unstructured data gives flexibility to developments on Microservices components. 
 CQRS Design Pattern CQRS stands for Command and Query Responsibility Segregation. It is one of the most widely used patterns for querying the database in Microservices architecture. CQRS is very handy when we need to eliminate complex queries involving inefficient joins. This pattern promotes the complete separation of read and write concerns in the database.  Traditionally, in Monolithic or SOA architecture, we have a single database for the entire application, and these databases will respond to both read and write requests. As the application becomes more complex over time, reading and writing to the database become non-performant. Sometimes, it has been observed that some applications follow an active-passive database model. However, even then, only one database copy is active at a time, so performance issues still persist.  For instance, in the case of database reads, if an application requires a query that needs to join more than ten tables, it can lead to locking the database due to the latency of query computation. Similarly, when writing to the database, we may need to perform complex validations and process lengthy business logic for some CRUD operations, which can also cause the locking of database operations.  So, reads and writes to the database are different operations for which separate strategies can be defined. To achieve this, CQRS offers to use “separation of concerns” principles and separate reads and writes into two databases. By this principle, we can use different databases for reading and writing database types, like using NoSQL for reading and using a relational database for CRUD operations.     
 So, another factor that is considered here is the nature of the application. If the use cases of your application mostly need to read the data in comparison to writing, then your application is a read-intensive application. So accordingly, you should focus 
Microservices Design Patterns    
 Copyright © 2023 ValueLabs. All rights reserved                                                                                                                   Page 7 of 17                                             
and choose your read and write databases. So here, CQRS separates reads and writes into different databases, where commands perform the creation or updating of data, and queries perform read data.  Commands are actions with some defined operations like “add the item to bucket” or “check my balance.” So, commands can be published via message brokers, which help applications process them in an async manner. Queries never modify the database. Queries always return the JSON data with DTO objects. In this way, we can isolate the Commands and Queries.  How to Sync Databases with CQRS? When we segregate read and write concerns in two different databases, the primary consideration is to sync these two data-bases properly. So, both databases should always be synced.  This can be achieved using Event Driven Architecture. As per Event Driven Architecture, when an update command is issued to the write database, it will publish an update event using message broker systems, and this will be consumed by the read database, which will pull the latest changes from the write database to keep the read database in sync.   But this option creates a consistency issue because the data would not be reflected immediately since async communication is implemented with message brokers. This will operate the principle of “eventual consistency.”  Eventual consistency is a property of distributed computing systems such that the value for a specific data item will, given enough time without up-dates, be consistent across all nodes. The read database eventually synchronizes with the write database, which will take some time to sync.  So, if we return to our read and write databases in the CQRS pattern when you start on the design, you can initially create a read database from the replicas of the write database. Also, as we have separate read and write databases, it means that both are scalable independently.     Event Sourcing  With CQRS, as reads and writes are separated, we need to keep both the database in sync as well, which is achieved using Event-Driven Architecture. But any failure in this sync or any loss of event can make data inconsistent in one of the databases, so to overcome this scenario, Event Sourcing is used. In Event Sourcing, apart from publishing the events on message broker systems, we store the events in the write database, and this event is stored in a single source of truth. In case of failure, events can be replayed, which will help keep data consistent.  Let's understand Event Sourcing with an example. Suppose there is a user’s table and a user updates their details. Usually, the updated details will override the existing values. This is generally how most applications work, i.e., they always store the entity’s current state.  But with the Event Sourcing pattern, it is changing. Instead of storing just the latest state, we store all operations in the database. The Event Sourcing pattern suggests saving all events into the database with a sequential order of data events. This events database is called an event store.  So, the event store is considered as a single source of truth for the data. After that, these event stores are converted to a read database using the materialized views. This conversion operation can also be handled by publish/subscribe patterns with events published through message broker systems. Moreover, this event list gives us the ability to replay events at a given timestamp. This allows us to build the latest status of data in case of any failure.  Let’s take the use case of a shopping cart for our e-commerce application, where we applied CQRS and Event Sourcing. As you can see in the image below, for the shopping cart, every user’s actions are recorded in the event store with appended events. All these events are combined and summarized on the read database with denormalized tables into materialized view database. Microservices Design Patterns    
 Copyright © 2023 ValueLabs. All rights reserved                                                                                                                   Page 8 of 17                                             
 By applying these patterns, we can query the latest status of the shopping cart using materialized views, which are used to create a read database. Instead of storing actual data, it stores sequential events that denote user actions, allowing us to know the history of actions the user has taken with timestamps. This enables us to retrieve the status of shopping cart data at any point in time. We can use event store write databases with Azure Cosmos DB, Cassandra, Event Store databases, etc.   SAGA   Usually, Microservices are designed around business domains, so the application is divided into multiple Microservices. However, there is a challenge, as now, any transaction will also be distributed across the services. To handle this distributed transaction scenario Saga pattern is used. 
   The Saga design pattern is used for managing data consistency in the case of distributed transactions. The Saga pattern tends to create a set of transactions that update each Microservice sequentially and publish an event to trigger the following transaction for the next Microservices. If the transaction fails at any step or service, then the Saga patterns trigger to rollback event, which basically does reverse operations in each of the Microservices.  A saga is a sequence of local transactions. Each local transaction updates the database and publishes a message or event to trigger the next local transaction in the saga. If a local transaction fails because it violates a business rule, then the saga executes a series of compensating transactions that undo the changes that were made by the preceding local transactions.   There are two options for how SAGA is implemented: • Choreography - Each local transaction publishes domain events that trigger local transactions in other services • Orchestration - An orchestrator (object) tells the participants what local transactions to execute      
Microservices Design Patterns    
 Copyright © 2023 ValueLabs. All rights reserved                                                                                                                   Page 9 of 17                                             
Choreography-based saga 
  Here is the e-commerce service using events to implement choreography-based SAGA.  Choreography enables the implementation of the saga pattern by applying publish-subscribe principles. With choreography, each Microservice executes its own local transactions and publishes events to a message broker system that triggers local transactions in other Microservices.  In the above diagram, the order service receives a post request for an order at step 1 and updates the local database. Then, it publishes the ‘order created’ event on the order event channel. At the same time, the customer service is a subscriber to the order event channel and will receive an event notification. On receiving notification, customer service triggers its own local tasks, like checking if the customer has appropriate credits or not and then reserving credits for the order for which the notification was received. Once done, it will either emit a ‘credit reserved’ or ‘credit limit exceeded' event in the customer event channel, which is subscribed by the order service. This is how the entire distributed transaction works.  For fewer services, this flow works well. Still, suppose the number of services are more. In that case, it becomes complex, and apart from its own local tasks, each service has the additional responsibility of emitting & consuming events as well, which makes things furthermore complex, and that’s where the Orchestration-based saga will solve the issue.  Orchestration Based Saga  
 
Microservices Design Patterns    
 Copyright © 2023 ValueLabs. All rights reserved                                                                                                                   Page 10 of 17                                             
 The Orchestrator-based saga also operates on events, but the responsibility for emitting events and ensuring the atomicity of transactions shifts from the services themselves to the orchestrator. Orchestrator commands services to execute local transactions. The orchestrator acts as an agent who tells services to execute local transactions and maintains the status of the complete transaction.  In the above diagram, the order service receives a post request for placing an order at step 1. Order service will initiate a transaction here and create a saga orchestrator. This orchestrator will ask the order service to create an order in the pending state. Now the orchestrator will send a “Reserve credit” command to customer service. Customer service will reserve credit and send the status of its local transaction channel, which the orchestrator consumes. Now, if the orchestrator receives a positive status from customer service, it will ask the order service to update the order status from “pending” to “In-Progress.” But, if the credit is not reserved, then the entire transaction will be rolled-back from the order and customer service. As the orchestrator now takes care of emitting events, this type of implementation is suitable for complex workflows involving many services. New services might also be added in the future. Since the orchestrator controls every transaction, there is less chance of getting into cyclical dependency.   Use the Saga pattern when you need to: • Ensure data consistency in a distributed system without tight coupling • Roll back or compensate if one of the operations in the sequence fails   BFF (Backend for Frontend) As and when an application is designed in Microservices architecture, there may be cases where data to be displayed in the frontend, be it web or mobile, might be coming from multiple services. In such cases, it becomes the responsibility of the frontend team to transform and aggregate responses from multiple Microservices. However, there are a few challenges with this approach:  • The frontend team needs to take care of a lot of transformations and aggregations • Also, the browser or mobile app interface will use more resources for rendering the page  BFF patterns solve these challenges by introducing an intermediate layer between frontend and backend services. The BFF layer acts as a proxy that will call all backend services, aggregate and transform the response as per the needs of the frontend, and expose ready-to-consume responses that best fit the frontend. Therefore, there will be very minimal logic on the frontend. Hence, a BFF helps to streamline the data formatting process and takes up the responsibility of providing a well-focused response for the frontend to consume.           Microservices Design Patterns    
 Copyright © 2023 ValueLabs. All rights reserved                                                                                                                   Page 11 of 17                                             
Here is an example showing BFF:  
  Ideally, the front-end team will be responsible for managing the BFF in most cases. A single BFF is focused on a single UI and that UI only. As a result, it will help us keep our frontends simple and see a unified view of data through its backend.  Usage of BFF again depends on the needs and architectural requirements. If the application you are designing or migrating to Microservices is simple, where each service has its own UI, then BFF is not needed. However, if the application has a complex functionality that involves multiple third-party integrations and front-end screens need to show processed data from multi-ple services, BFF is the best fit. Also, if the client’s requirement is around optimal rendering of the frontend and backend, and you have complex Microservices, a BFF is suitable to use.  Furthermore, there can be multiple BFFs in the application depending on the needs and requirements of the application. In a scenario where the same backend code serves both web and mobile, where each shows data differently, there can be one BFF layer for the web and one BFF layer for mobile.  
  To conclude, implementing these patterns should be done judiciously, and code duplication should be avoided.    
Microservices Design Patterns    
 Copyright © 2023 ValueLabs. All rights reserved                                                                                                                   Page 12 of 17                                             
API Gateway If we need to build a large, complex Microservices-based application with multiple clients, we can use the API gateway pattern. This pattern applies to distributed systems and acts as a reverse proxy or gateway routing for requests coming in from clients. API gateway acts as communication between the client and underlying services and serves as a single-entry point to the system while abstracting the underlying architecture. It also provides cross-cutting concerns like authentication, SSL termination, and caching. 
 So, as you can see, the API gateway provides a single entry point to multiple services. There are some things that we need to make sure of while using this pattern.   If there is only one node acting as an API gateway, there is a chance it will become a single point of failure. This situation needs to be handled, as if more complex logic is added to the API Gateway, it will become an anti-pattern. As the usage of this pattern grows,  it is advised to deploy multiple API Gateways for multiple services based on the use case and the number of services.  In summary, we need to be cautious about using a single API Gateway, as it should be decided based on the business context of the client applications. Ideally, it is not advised to have one API gateway for all the internal Microservices.   Strangler Pattern The Strangler Pattern is a methodology used in Microservices architecture to gradually migrate a Monolithic application to a Microservices-based architecture. This pattern is useful when the application is too large and complex to migrate simultane-ously.  The Strangler Pattern involves designing a new set of Microservices that gradually take over the functionality of the Mono-lithic application. Initially, all requests for specific functionality are divided between the traditional Monolith and the new Microservice to cater to the same functionality. Over time, as the Microservices mature, they start to handle more of the traffic, and the Monolithic application handles less. Slowly, the entire complex Monolith will be replaced by more agile and scalable Microservices.   
Microservices Design Patterns    
 Copyright © 2023 ValueLabs. All rights reserved                                                                                                                   Page 13 of 17                                             
 The Strangler Pattern follows these basic steps: Ø Identify the functionality of the Monolithic application around the business domain that can be converted to a                  Microservice Ø Create a new set of Microservices from scratch to handle the identified functionality Ø Then, we must configure partial traffic to the new Microservices using an API gateway or any proxy server Ø Over a period, gradually increase the traffic to the Microservices while decreasing the traffic to the Monolithic application Ø Keep monitoring the performance of the new Microservices and adjust the traffic accordingly Ø Eventually, route entire traffic to Microservices and bring down the entire Monolithic service  Circuit Breaker The Circuit Breaker Pattern is a technique used in Microservices architecture to improve the resilience and reliability of dis-tributed systems. The basic idea behind the circuit breaker is very simple. You wrap a function call in a circuit breaker object, which monitors it for failures. Once the failures reach a certain threshold, the circuit breaker trips and all further calls to the circuit breaker return an error, with any further calls skipped. The Circuit Breaker can also be configured to handle failures in different ways, such as returning a default value or a cached response.  
Microservices Design Patterns    
 Copyright © 2023 ValueLabs. All rights reserved                                                                                                                   Page 14 of 17                                             
                           Here are the basic steps involved in the Circuit Breaker Pattern: Ø Identify the Microservices that need to communicate with each other Ø Add a Circuit Breaker between the Microservices Ø Add configurations to monitor the status of the Microservices and detect failures Ø If a Microservice fails to respond, the Circuit Breaker can prevent further requests to the Microservice and handle the fail-ure in an appropriate way Ø In some cases, the default response is also configured, which will help to handle failures gracefully.  Externalized Configuration In Microservices architectures, systems are divided into multiple Microservices, each one running in its own container. Each process can be deployed and independently scaled, which means there may be many instances of the same microservice running at a specific time.  Let’s say we want to modify the configurations for a microservice that has been replicated almost a hundred times.  If the configuration for this microservice is packaged with the service itself, we need to do deployment again for each in-stance running. This can lead to an inconsistent state as there are high chances that a few instances still need to upgrade to a new build and are still running on old configurations. Therefore, it is advised that services share the external configuration.  
Microservices Design Patterns    
 Copyright © 2023 ValueLabs. All rights reserved                                                                                                                   Page 15 of 17                                             
 It works by keeping the configurations in an external store, such as a database, file system, or environment variables. When the Microservice is deployed and started, it loads the configuration from the external source. At runtime, if configuration changes occur, those are generally reloaded by Microservices without any new deployments.  Consumer-Driven Contract Tracing This pattern can be referred to as the test-driven development pattern for the development of Microservices. The pattern suggests taking a design-first approach where negotiation on the expected response between provider and consumer de-cides the best possible outcome. The developers of a consumer service write “contracts” specifying the responses they ex-pect from the requests made to a service provider. It is “consumer-driven” because the consumer’s developers drive the writing of the contract and lead the negotiations with the provider’s developers.   The contract is typically a JSON or XML file that the developers and their services (consumer and provider) can access. One of the main benefits of consumer-driven contract testing is that it allows collaboration from the start between service providers and consumers. It also helps service developers to understand the requirements and expectations of consumers in a proac-tive manner.   Another benefit of CDCT is that it helps to prevent integration issues between services. By defining the contract up front, both the consumer and provider can ensure they are on the same page, saving many testing cycles as well.  Here are the steps to implement CDCT: 1. Define the contract: The consumer of a service should define the contract that the service provider must follow. This should include the expected input and output of the service, as well as any other requirements that the consumer has. 2. Implement the contract: The service provider should implement the contract and ensure that their service meets the re-quirements.  3. Run tests: The service provider should then run test cases to ensure the service meets the requirements. This can include both unit tests and integration tests.  4. Publish the contract: Once the service provider has implemented the contract and passed their tests, they should             publish the contract to the consumer.  
Microservices Design Patterns    
 Copyright © 2023 ValueLabs. All rights reserved                                                                                                                   Page 16 of 17                                             
5. Verify compatibility: Consumers can then verify that they got what was agreed upon. 
Conclusion With the increase in the usage of Microservices-based architecture, complexities arise in managing scalability or handling distributed services' transactions. However, a set of defined patterns, which have been tested repeatedly, give solutions to problems that are very common for Microservice-based architectures. Knowing each pattern provides good insight into how Microservices architecture handles performance, scalability, agility, and maintainability. We hope that the few patterns de-scribed above provide good insight for you.           
Microservices Design Patterns    
 Copyright © 2023 ValueLabs. All rights reserved                                                                                                                   Page 17 of 17                                             
About the Author  
  Nishant Malhotra is working as an Architect - Digital Consulting at ValueLabs. He has over 13 years of experience in deliver-ing enterprise solutions in e-commerce, m-commerce, workflow, and web-based areas. He is also well-versed in executing solutions for complex business problems involving large-scale data handling, real-time analytics, and reporting solutions. His other areas of expertise include defining architecture, designing, and technical coding solutions using Java/J2EE and cloud infrastructure.  Nishant can be reached at nishant.malhotra@valuelabs.com       

2025-02-05 14:12:56,459 - INFO - hello
2025-02-05 14:12:56,459 - INFO - Text extracted from pdf and converted into chunks
2025-02-05 14:13:04,381 - INFO - (Collection(name=Microservices-Design-Patterns.pdf), 'Microservices-Design-Patterns.pdf')
2025-02-05 14:13:04,396 - ERROR - Failed to get relevent passage, Please change the query
2025-02-05 14:13:04,396 - ERROR - Failed to perform db operations 'tuple' object has no attribute 'query'
2025-02-05 14:14:27,602 - INFO - db configuration is localhost, 8000
2025-02-05 14:14:27,769 - INFO - successfully connected to chromadb
2025-02-05 14:14:31,505 - INFO -   
      
www.valuelabs.com 
Microservices  Design Patterns  Microservices Design Patterns    
 Copyright © 2023 ValueLabs. All rights reserved                                                                                                                   Page 1 of 17                                             
Table of Contents Introduction ........................................................................................................................................ 2 Abstract .............................................................................................................................................. 2 Introduction to Microservices .............................................................................................................. 2 Microservices Design Patterns and Principles ...................................................................................... 3 Database per Microservices ................................................................................................................ 5 Database changes can be made independently without impacting other Microservices: ............................... 5 CQRS Design Pattern ........................................................................................................................... 6 How to Sync Databases with CQRS? .............................................................................................................. 7 Event Sourcing .................................................................................................................................... 7 SAGA ................................................................................................................................................... 8 Choreography-based saga ............................................................................................................................. 9 Orchestration Based Saga ............................................................................................................................. 9 BFF (Backend for Frontend) ............................................................................................................... 10 Here is an example showing BFF: ................................................................................................................ 11 API Gateway ..................................................................................................................................... 12 Strangler Pattern .............................................................................................................................. 12 The Strangler Pattern follows these basic steps: .......................................................................................... 13 Circuit Breaker .................................................................................................................................. 13 Here are the basic steps involved in the Circuit Breaker Pattern: ................................................................. 14 Externalized Configuration ................................................................................................................ 14 Consumer-Driven Contract Tracing .................................................................................................... 15 Here are the steps to implement CDCT: ....................................................................................................... 15 Conclusion ......................................................................................................................................... 16         Microservices Design Patterns    
 Copyright © 2023 ValueLabs. All rights reserved                                                                                                                   Page 2 of 17                                             
  Introduction The purpose of this guide is to help you with architectural patterns followed while designing Microservices architecture.  Abstract This document aims to provide basic know-how of the ‘Microservices Design Patterns.’ We aim to provide a reference document for design patterns to be used when any application is designed using Microservices architecture.   We aim to publish this whitepaper as a ready reference to ten highly used design patterns related to Microservices in a detailed fashion.  Introduction to Microservices Microservices are self-contained and independent deployment modules. In Microservices, the application is divided into independent modules based on business domains. Microservices are designed based on Domain Driven Design (DDD), which says the application should be modeled around independent modules with bounded context for the specific business do-main. As we now have independent modules, we have faster rollout and quicker release cycles. All the modules can be worked on in parallel, and any changes will only affect that specific module. Therefore, Continuous Integration (CI) & Continu-ous Delivery (CD) are key advantages of using Microservices architecture.     As Monolithic applications pose their own set of challenges, here are four main concepts that describe the importance of Microservices architecture over Monolithic architecture. Some of the key points about Microservices are:  1. Loosely coupled components designed around business domains 2. Application can be distributed across different clouds and data centers 3. Change management is easy in Microservices 
In short, the Microservices architectural style is an approach to developing a single application as a suite of small services, each running in its own process and communicating with lightweight mechanisms, often an HTTP resource API. These services are built around business capabilities and are independently deployable by fully automated deployment machinery.   – Martin Fowler Microservices Design Patterns    
 Copyright © 2023 ValueLabs. All rights reserved                                                                                                                   Page 3 of 17                                             
  Microservices Design Patterns and Principles As we now know, Microservices are independent components that encapsulate business domains. If applications are archi-tected using Microservices design principles and patterns, then the application is highly scalable. So, let's list down the princi-ples in which the Microservices architecture has been built. 1. Scalability 2. Flexibility 3. Independent and autonomous 4. Decentralized governance 5. Resiliency 6. Failure isolation 7. Continuous delivery through the DevOps While adhering to the principles mentioned above, there are some standard sets of problems & complex scenarios that archi-tects and developers need to address. In addressing such common problems, statements bring proven solutions to the table, which become patterns. Here are a few high-level patterns that are used around Microservices.  
Microservices Design Patterns    
 Copyright © 2023 ValueLabs. All rights reserved                                                                                                                   Page 4 of 17                                             
                                         Picture Reference: https://microservices.io/patterns/microservices.html We will cover the most used and discussed design patterns in detail, along with use cases. Here is the list of design patterns that will be covered: 1. Database per Microservices 2. CQRS 3. Event Sourcing 4. Saga 5. BFF 6. API Gateway 7. Strangler 8. Circuit Breaker 9. Externalized Configuration 10. Consumer-Driven Contract Tracing    
Microservices Design Patterns    
 Copyright © 2023 ValueLabs. All rights reserved                                                                                                                   Page 5 of 17                                             
Database per Microservices    
 Microservices are generally independent and loosely bound components. So, to achieve this independent nature, every Microservice must have its own database so that it can be developed and deployed independently.   Let’s take the example of an e-commerce application. We will have product, ordering and SKU Microservices that each service interacts (store and retrieve) with data from their own databases. Any changes to one database don’t impact other Microservices.  Other Microservices can’t directly access each independent database. Each component’s persistent data can only be ac-cessed via APIs. So, Database per Microservices provides many benefits, especially to evolve rapidly and support massive scaling as they make each Microservice independent.   Database changes can be made independently without impacting other Microservices: • Each database can scale independently • Microservices domain data is encapsulated within the service • If one of the database servers is down, this will not affect other services Also *Polyglot data persistence gives the ability to select the best-optimized storage needs per Microservices. So, we can use this as an example - an e-commerce application with Microservices, as mentioned earlier. Here are the optimized choices: • The product Microservices use a NoSQL document database for storing catalog-related data, which is storing JSON objects to accommodate high volumes of read operations • The shopping cart Microservices use a distributed cache that supports its simple, key-value datastore • The ordering Microservices use a relational database to accommodate the rich relational structure of its underlying data 
Microservices Design Patterns    
 Copyright © 2023 ValueLabs. All rights reserved                                                                                                                   Page 6 of 17                                             
 Because of the ability to scale and high availability, NoSQL databases are getting highly popular and are widely used in enter-prise applications nowadays. Also, their support for unstructured data gives flexibility to developments on Microservices components. 
 CQRS Design Pattern CQRS stands for Command and Query Responsibility Segregation. It is one of the most widely used patterns for querying the database in Microservices architecture. CQRS is very handy when we need to eliminate complex queries involving inefficient joins. This pattern promotes the complete separation of read and write concerns in the database.  Traditionally, in Monolithic or SOA architecture, we have a single database for the entire application, and these databases will respond to both read and write requests. As the application becomes more complex over time, reading and writing to the database become non-performant. Sometimes, it has been observed that some applications follow an active-passive database model. However, even then, only one database copy is active at a time, so performance issues still persist.  For instance, in the case of database reads, if an application requires a query that needs to join more than ten tables, it can lead to locking the database due to the latency of query computation. Similarly, when writing to the database, we may need to perform complex validations and process lengthy business logic for some CRUD operations, which can also cause the locking of database operations.  So, reads and writes to the database are different operations for which separate strategies can be defined. To achieve this, CQRS offers to use “separation of concerns” principles and separate reads and writes into two databases. By this principle, we can use different databases for reading and writing database types, like using NoSQL for reading and using a relational database for CRUD operations.     
 So, another factor that is considered here is the nature of the application. If the use cases of your application mostly need to read the data in comparison to writing, then your application is a read-intensive application. So accordingly, you should focus 
Microservices Design Patterns    
 Copyright © 2023 ValueLabs. All rights reserved                                                                                                                   Page 7 of 17                                             
and choose your read and write databases. So here, CQRS separates reads and writes into different databases, where commands perform the creation or updating of data, and queries perform read data.  Commands are actions with some defined operations like “add the item to bucket” or “check my balance.” So, commands can be published via message brokers, which help applications process them in an async manner. Queries never modify the database. Queries always return the JSON data with DTO objects. In this way, we can isolate the Commands and Queries.  How to Sync Databases with CQRS? When we segregate read and write concerns in two different databases, the primary consideration is to sync these two data-bases properly. So, both databases should always be synced.  This can be achieved using Event Driven Architecture. As per Event Driven Architecture, when an update command is issued to the write database, it will publish an update event using message broker systems, and this will be consumed by the read database, which will pull the latest changes from the write database to keep the read database in sync.   But this option creates a consistency issue because the data would not be reflected immediately since async communication is implemented with message brokers. This will operate the principle of “eventual consistency.”  Eventual consistency is a property of distributed computing systems such that the value for a specific data item will, given enough time without up-dates, be consistent across all nodes. The read database eventually synchronizes with the write database, which will take some time to sync.  So, if we return to our read and write databases in the CQRS pattern when you start on the design, you can initially create a read database from the replicas of the write database. Also, as we have separate read and write databases, it means that both are scalable independently.     Event Sourcing  With CQRS, as reads and writes are separated, we need to keep both the database in sync as well, which is achieved using Event-Driven Architecture. But any failure in this sync or any loss of event can make data inconsistent in one of the databases, so to overcome this scenario, Event Sourcing is used. In Event Sourcing, apart from publishing the events on message broker systems, we store the events in the write database, and this event is stored in a single source of truth. In case of failure, events can be replayed, which will help keep data consistent.  Let's understand Event Sourcing with an example. Suppose there is a user’s table and a user updates their details. Usually, the updated details will override the existing values. This is generally how most applications work, i.e., they always store the entity’s current state.  But with the Event Sourcing pattern, it is changing. Instead of storing just the latest state, we store all operations in the database. The Event Sourcing pattern suggests saving all events into the database with a sequential order of data events. This events database is called an event store.  So, the event store is considered as a single source of truth for the data. After that, these event stores are converted to a read database using the materialized views. This conversion operation can also be handled by publish/subscribe patterns with events published through message broker systems. Moreover, this event list gives us the ability to replay events at a given timestamp. This allows us to build the latest status of data in case of any failure.  Let’s take the use case of a shopping cart for our e-commerce application, where we applied CQRS and Event Sourcing. As you can see in the image below, for the shopping cart, every user’s actions are recorded in the event store with appended events. All these events are combined and summarized on the read database with denormalized tables into materialized view database. Microservices Design Patterns    
 Copyright © 2023 ValueLabs. All rights reserved                                                                                                                   Page 8 of 17                                             
 By applying these patterns, we can query the latest status of the shopping cart using materialized views, which are used to create a read database. Instead of storing actual data, it stores sequential events that denote user actions, allowing us to know the history of actions the user has taken with timestamps. This enables us to retrieve the status of shopping cart data at any point in time. We can use event store write databases with Azure Cosmos DB, Cassandra, Event Store databases, etc.   SAGA   Usually, Microservices are designed around business domains, so the application is divided into multiple Microservices. However, there is a challenge, as now, any transaction will also be distributed across the services. To handle this distributed transaction scenario Saga pattern is used. 
   The Saga design pattern is used for managing data consistency in the case of distributed transactions. The Saga pattern tends to create a set of transactions that update each Microservice sequentially and publish an event to trigger the following transaction for the next Microservices. If the transaction fails at any step or service, then the Saga patterns trigger to rollback event, which basically does reverse operations in each of the Microservices.  A saga is a sequence of local transactions. Each local transaction updates the database and publishes a message or event to trigger the next local transaction in the saga. If a local transaction fails because it violates a business rule, then the saga executes a series of compensating transactions that undo the changes that were made by the preceding local transactions.   There are two options for how SAGA is implemented: • Choreography - Each local transaction publishes domain events that trigger local transactions in other services • Orchestration - An orchestrator (object) tells the participants what local transactions to execute      
Microservices Design Patterns    
 Copyright © 2023 ValueLabs. All rights reserved                                                                                                                   Page 9 of 17                                             
Choreography-based saga 
  Here is the e-commerce service using events to implement choreography-based SAGA.  Choreography enables the implementation of the saga pattern by applying publish-subscribe principles. With choreography, each Microservice executes its own local transactions and publishes events to a message broker system that triggers local transactions in other Microservices.  In the above diagram, the order service receives a post request for an order at step 1 and updates the local database. Then, it publishes the ‘order created’ event on the order event channel. At the same time, the customer service is a subscriber to the order event channel and will receive an event notification. On receiving notification, customer service triggers its own local tasks, like checking if the customer has appropriate credits or not and then reserving credits for the order for which the notification was received. Once done, it will either emit a ‘credit reserved’ or ‘credit limit exceeded' event in the customer event channel, which is subscribed by the order service. This is how the entire distributed transaction works.  For fewer services, this flow works well. Still, suppose the number of services are more. In that case, it becomes complex, and apart from its own local tasks, each service has the additional responsibility of emitting & consuming events as well, which makes things furthermore complex, and that’s where the Orchestration-based saga will solve the issue.  Orchestration Based Saga  
 
Microservices Design Patterns    
 Copyright © 2023 ValueLabs. All rights reserved                                                                                                                   Page 10 of 17                                             
 The Orchestrator-based saga also operates on events, but the responsibility for emitting events and ensuring the atomicity of transactions shifts from the services themselves to the orchestrator. Orchestrator commands services to execute local transactions. The orchestrator acts as an agent who tells services to execute local transactions and maintains the status of the complete transaction.  In the above diagram, the order service receives a post request for placing an order at step 1. Order service will initiate a transaction here and create a saga orchestrator. This orchestrator will ask the order service to create an order in the pending state. Now the orchestrator will send a “Reserve credit” command to customer service. Customer service will reserve credit and send the status of its local transaction channel, which the orchestrator consumes. Now, if the orchestrator receives a positive status from customer service, it will ask the order service to update the order status from “pending” to “In-Progress.” But, if the credit is not reserved, then the entire transaction will be rolled-back from the order and customer service. As the orchestrator now takes care of emitting events, this type of implementation is suitable for complex workflows involving many services. New services might also be added in the future. Since the orchestrator controls every transaction, there is less chance of getting into cyclical dependency.   Use the Saga pattern when you need to: • Ensure data consistency in a distributed system without tight coupling • Roll back or compensate if one of the operations in the sequence fails   BFF (Backend for Frontend) As and when an application is designed in Microservices architecture, there may be cases where data to be displayed in the frontend, be it web or mobile, might be coming from multiple services. In such cases, it becomes the responsibility of the frontend team to transform and aggregate responses from multiple Microservices. However, there are a few challenges with this approach:  • The frontend team needs to take care of a lot of transformations and aggregations • Also, the browser or mobile app interface will use more resources for rendering the page  BFF patterns solve these challenges by introducing an intermediate layer between frontend and backend services. The BFF layer acts as a proxy that will call all backend services, aggregate and transform the response as per the needs of the frontend, and expose ready-to-consume responses that best fit the frontend. Therefore, there will be very minimal logic on the frontend. Hence, a BFF helps to streamline the data formatting process and takes up the responsibility of providing a well-focused response for the frontend to consume.           Microservices Design Patterns    
 Copyright © 2023 ValueLabs. All rights reserved                                                                                                                   Page 11 of 17                                             
Here is an example showing BFF:  
  Ideally, the front-end team will be responsible for managing the BFF in most cases. A single BFF is focused on a single UI and that UI only. As a result, it will help us keep our frontends simple and see a unified view of data through its backend.  Usage of BFF again depends on the needs and architectural requirements. If the application you are designing or migrating to Microservices is simple, where each service has its own UI, then BFF is not needed. However, if the application has a complex functionality that involves multiple third-party integrations and front-end screens need to show processed data from multi-ple services, BFF is the best fit. Also, if the client’s requirement is around optimal rendering of the frontend and backend, and you have complex Microservices, a BFF is suitable to use.  Furthermore, there can be multiple BFFs in the application depending on the needs and requirements of the application. In a scenario where the same backend code serves both web and mobile, where each shows data differently, there can be one BFF layer for the web and one BFF layer for mobile.  
  To conclude, implementing these patterns should be done judiciously, and code duplication should be avoided.    
Microservices Design Patterns    
 Copyright © 2023 ValueLabs. All rights reserved                                                                                                                   Page 12 of 17                                             
API Gateway If we need to build a large, complex Microservices-based application with multiple clients, we can use the API gateway pattern. This pattern applies to distributed systems and acts as a reverse proxy or gateway routing for requests coming in from clients. API gateway acts as communication between the client and underlying services and serves as a single-entry point to the system while abstracting the underlying architecture. It also provides cross-cutting concerns like authentication, SSL termination, and caching. 
 So, as you can see, the API gateway provides a single entry point to multiple services. There are some things that we need to make sure of while using this pattern.   If there is only one node acting as an API gateway, there is a chance it will become a single point of failure. This situation needs to be handled, as if more complex logic is added to the API Gateway, it will become an anti-pattern. As the usage of this pattern grows,  it is advised to deploy multiple API Gateways for multiple services based on the use case and the number of services.  In summary, we need to be cautious about using a single API Gateway, as it should be decided based on the business context of the client applications. Ideally, it is not advised to have one API gateway for all the internal Microservices.   Strangler Pattern The Strangler Pattern is a methodology used in Microservices architecture to gradually migrate a Monolithic application to a Microservices-based architecture. This pattern is useful when the application is too large and complex to migrate simultane-ously.  The Strangler Pattern involves designing a new set of Microservices that gradually take over the functionality of the Mono-lithic application. Initially, all requests for specific functionality are divided between the traditional Monolith and the new Microservice to cater to the same functionality. Over time, as the Microservices mature, they start to handle more of the traffic, and the Monolithic application handles less. Slowly, the entire complex Monolith will be replaced by more agile and scalable Microservices.   
Microservices Design Patterns    
 Copyright © 2023 ValueLabs. All rights reserved                                                                                                                   Page 13 of 17                                             
 The Strangler Pattern follows these basic steps: Ø Identify the functionality of the Monolithic application around the business domain that can be converted to a                  Microservice Ø Create a new set of Microservices from scratch to handle the identified functionality Ø Then, we must configure partial traffic to the new Microservices using an API gateway or any proxy server Ø Over a period, gradually increase the traffic to the Microservices while decreasing the traffic to the Monolithic application Ø Keep monitoring the performance of the new Microservices and adjust the traffic accordingly Ø Eventually, route entire traffic to Microservices and bring down the entire Monolithic service  Circuit Breaker The Circuit Breaker Pattern is a technique used in Microservices architecture to improve the resilience and reliability of dis-tributed systems. The basic idea behind the circuit breaker is very simple. You wrap a function call in a circuit breaker object, which monitors it for failures. Once the failures reach a certain threshold, the circuit breaker trips and all further calls to the circuit breaker return an error, with any further calls skipped. The Circuit Breaker can also be configured to handle failures in different ways, such as returning a default value or a cached response.  
Microservices Design Patterns    
 Copyright © 2023 ValueLabs. All rights reserved                                                                                                                   Page 14 of 17                                             
                           Here are the basic steps involved in the Circuit Breaker Pattern: Ø Identify the Microservices that need to communicate with each other Ø Add a Circuit Breaker between the Microservices Ø Add configurations to monitor the status of the Microservices and detect failures Ø If a Microservice fails to respond, the Circuit Breaker can prevent further requests to the Microservice and handle the fail-ure in an appropriate way Ø In some cases, the default response is also configured, which will help to handle failures gracefully.  Externalized Configuration In Microservices architectures, systems are divided into multiple Microservices, each one running in its own container. Each process can be deployed and independently scaled, which means there may be many instances of the same microservice running at a specific time.  Let’s say we want to modify the configurations for a microservice that has been replicated almost a hundred times.  If the configuration for this microservice is packaged with the service itself, we need to do deployment again for each in-stance running. This can lead to an inconsistent state as there are high chances that a few instances still need to upgrade to a new build and are still running on old configurations. Therefore, it is advised that services share the external configuration.  
Microservices Design Patterns    
 Copyright © 2023 ValueLabs. All rights reserved                                                                                                                   Page 15 of 17                                             
 It works by keeping the configurations in an external store, such as a database, file system, or environment variables. When the Microservice is deployed and started, it loads the configuration from the external source. At runtime, if configuration changes occur, those are generally reloaded by Microservices without any new deployments.  Consumer-Driven Contract Tracing This pattern can be referred to as the test-driven development pattern for the development of Microservices. The pattern suggests taking a design-first approach where negotiation on the expected response between provider and consumer de-cides the best possible outcome. The developers of a consumer service write “contracts” specifying the responses they ex-pect from the requests made to a service provider. It is “consumer-driven” because the consumer’s developers drive the writing of the contract and lead the negotiations with the provider’s developers.   The contract is typically a JSON or XML file that the developers and their services (consumer and provider) can access. One of the main benefits of consumer-driven contract testing is that it allows collaboration from the start between service providers and consumers. It also helps service developers to understand the requirements and expectations of consumers in a proac-tive manner.   Another benefit of CDCT is that it helps to prevent integration issues between services. By defining the contract up front, both the consumer and provider can ensure they are on the same page, saving many testing cycles as well.  Here are the steps to implement CDCT: 1. Define the contract: The consumer of a service should define the contract that the service provider must follow. This should include the expected input and output of the service, as well as any other requirements that the consumer has. 2. Implement the contract: The service provider should implement the contract and ensure that their service meets the re-quirements.  3. Run tests: The service provider should then run test cases to ensure the service meets the requirements. This can include both unit tests and integration tests.  4. Publish the contract: Once the service provider has implemented the contract and passed their tests, they should             publish the contract to the consumer.  
Microservices Design Patterns    
 Copyright © 2023 ValueLabs. All rights reserved                                                                                                                   Page 16 of 17                                             
5. Verify compatibility: Consumers can then verify that they got what was agreed upon. 
Conclusion With the increase in the usage of Microservices-based architecture, complexities arise in managing scalability or handling distributed services' transactions. However, a set of defined patterns, which have been tested repeatedly, give solutions to problems that are very common for Microservice-based architectures. Knowing each pattern provides good insight into how Microservices architecture handles performance, scalability, agility, and maintainability. We hope that the few patterns de-scribed above provide good insight for you.           
Microservices Design Patterns    
 Copyright © 2023 ValueLabs. All rights reserved                                                                                                                   Page 17 of 17                                             
About the Author  
  Nishant Malhotra is working as an Architect - Digital Consulting at ValueLabs. He has over 13 years of experience in deliver-ing enterprise solutions in e-commerce, m-commerce, workflow, and web-based areas. He is also well-versed in executing solutions for complex business problems involving large-scale data handling, real-time analytics, and reporting solutions. His other areas of expertise include defining architecture, designing, and technical coding solutions using Java/J2EE and cloud infrastructure.  Nishant can be reached at nishant.malhotra@valuelabs.com       

2025-02-05 14:14:31,509 - INFO - ["  \n      \nwww.valuelabs.com \nMicroservices  Design Patterns  Microservices Design Patterns    \n Copyright © 2023 ValueLabs. All rights reserved                                                                                                                   Page 1 of 17                                             \nTable of Contents Introduction ........................................................................................................................................ 2 Abstract .............................................................................................................................................. 2 Introduction to Microservices .............................................................................................................. 2 Microservices Design Patterns and Principles ...................................................................................... 3 Database per Microservices ................................................................................................................ 5 Database changes can be made independently without impacting other Microservices: ............................... 5 CQRS Design Pattern ........................................................................................................................... 6 How to Sync Databases with CQRS? .............................................................................................................. 7 Event Sourcing .................................................................................................................................... 7 SAGA ................................................................................................................................................... 8 Choreography-based saga ............................................................................................................................. 9 Orchestration Based Saga ............................................................................................................................. 9 BFF (Backend for Frontend) ............................................................................................................... 10 Here is an example showing BFF: ................................................................................................................ 11 API Gateway ..................................................................................................................................... 12 Strangler Pattern .............................................................................................................................. 12 The Strangler Pattern follows these basic steps: .......................................................................................... 13 Circuit Breaker .................................................................................................................................. 13 Here are the basic steps involved in the Circuit Breaker Pattern: ................................................................. 14 Externalized Configuration ................................................................................................................ 14 Consumer-Driven Contract Tracing .................................................................................................... 15 Here are the steps to implement CDCT: ....................................................................................................... 15 Conclusion ......................................................................................................................................... 16         Microservices Design Patterns    \n Copyright © 2023 ValueLabs. All rights reserved                                                                                                                   Page 2 of 17                                             \n  Introduction The purpose of this guide is to help you with architectural patterns followed while designing Microservices architecture.  Abstract This document aims to provide basic know-how of the ‘Microservices Design Patterns.’ We aim to provide a reference document for design patterns to be used when any application is designed using Microservices architecture.   We aim to publish this whitepaper as a ready reference to ten highly used design patterns related to Microservices in a detailed fashion.  Introduction to Microservices Microservices are self-contained and independent deployment modules. In Microservices, the application is divided into independent modules based on business domains. Microservices are designed based on Domain Driven Design (DDD), which says the application should be modeled around independent modules with bounded context for the specific business do-main. As we now have independent modules, we have faster rollout and quicker release cycles. All the modules can be worked on in parallel, and any changes will only affect that specific module. Therefore, Continuous Integration (CI) & Continu-ous Delivery (CD) are key advantages of using Microservices architecture.     As Monolithic applications pose their own set of challenges, here are four main concepts that describe the importance of Microservices architecture over Monolithic architecture. Some of the key points about Microservices are:  1. Loosely coupled components designed around business domains 2. Application can be distributed across different clouds and data centers 3. Change management is easy in Microservices \nIn short, the Microservices architectural style is an approach to developing a single application as a suite of small services, each running in its own process and communicating with lightweight mechanisms, often an HTTP resource API. These services are built around business capabilities and are independently deployable by fully automated deployment machinery.   – Martin Fowler Microservices Design Patterns    \n Copyright © 2023 ValueLabs. All rights reserved                                                                                                                   Page 3 of 17                                             \n  Microservices Design Patterns and Principles As we now know, Microservices are independent components that encapsulate business domains. If applications are archi-tected using Microservices design principles and patterns, then the application is highly scalable. So, let's list down the princi-ples in which the Microservices architecture has been built. 1. Scalability 2. Flexibility 3. Independent and autonomous 4. Decentralized governance 5. Resiliency 6. Failure isolation 7. Continuous delivery through the DevOps While adhering to the principles mentioned above, there are some standard sets of problems & complex scenarios that archi-tects and developers need to address. In addressing such common problems, statements bring proven solutions to the table, which become patterns. Here are a few high-level patterns that are used around Microservices.  \nMicroservices Design Patterns    \n Copyright © 2023 ValueLabs. All rights reserved                                                                                                                   Page 4 of 17                                             \n                                         Picture Reference: https://microservices.io/patterns/microservices.html We will cover the most used and discussed design patterns in detail, along with use cases. Here is the list of design patterns that will be covered: 1. Database per Microservices 2. CQRS 3. Event Sourcing 4. Saga 5. BFF 6. API Gateway 7. Strangler 8. Circuit Breaker 9. Externalized Configuration 10. Consumer-Driven Contract Tracing    \nMicroservices Design Patterns    \n Copyright © 2023 ValueLabs. All rights reserved                                                                                                                   Page 5 of 17                                             \nDatabase per Microservices    \n Microservices are generally independent and loosely bound components. So, to achieve this independent nature, every Microservice must have its own database so that it can be developed and deployed independently.   Let’s take the example of an e-commerce application. We will have product, ordering and SKU Microservices that each service interacts (store and retrieve) with data from their own databases. Any changes to one database don’t impact other Microservices.  Other Microservices can’t directly access each independent database. Each component’s persistent data can only be ac-cessed via APIs. So, Database per Microservices provides many benefits, especially to evolve rapidly and support massive scaling as they make each Microservice independent.   Database changes can be made independently without impacting other Microservices: • Each database can scale independently • Microservices domain data is encapsulated within the service • If one of the database servers is down, this will not affect other services Also *Polyglot data persistence gives the ability to select the best-optimized storage needs per Microservices. So, we can use this as an example - an e-commerce application with Microservices, as mentioned earlier. Here are the optimized choices: • The product Microservices use a NoSQL document database for storing catalog-related data, which is storing JSON objects to accommodate high volumes of read operations • The shopping cart Microservices use a distributed cache that supports its simple, key-value datastore • The ordering Microservices use a relational database to accommodate the rich relational structure of its underlying data \nMicroservices Design Patterns    \n Copyright © 2023 ValueLabs. All rights reserved                                                                                                                   Page 6 of 17                                             \n Because of the ability to scale and high availability, NoSQL databases are getting highly popular and are widely used in enter-prise applications nowadays. Also, their support for unstructured data gives flexibility to developments on Microservices components. \n CQRS Design Pattern CQRS stands for Command and Query Responsibility Segregation. It is one of the most widely used patterns for querying the database in Microservices architecture. CQRS is very handy when we need to eliminate complex queries involving inefficient joins. This pattern promotes the complete separation of read and write concerns in the database.  Traditionally, in Monolithic or SOA architecture, we have a single database for the entire application, and these databases will respond to both read and write requests. As the application becomes more complex over time, reading and writing to the database become non-performant. Sometimes, it has been observed that some applications follow an active-passive database model. However, even then, only one database copy is active at a time, so performance issues still persist.  For instance, in the case of database reads, if an application requires a query that needs to join more than ten tables, it can lead to locking the database due to the latency of query computation. Similarly, when writing to the database, we may need to perform complex validations and process lengthy business logic for some CRUD operations, which can also cause the locking of database operations.  So, reads and writes to the database are different operations for which separate strategies can be defined. To achieve this, CQRS offers to use “separation of concerns” principles and separate reads and writes into two databases. By this principle, we can use different databases for reading and writing database types, like using NoSQL for reading and using a relational database for CRUD operations.     \n So, another factor that is considered here is the nature of the application. If the use cases of your application mostly need to read the data in comparison to writing, then your application is a read-intensive application. So accordingly, you should focus \nMicroservices Design Patterns    \n Copyright © 2023 ValueLabs. All rights reserved                                                                                                                   Page 7 of 17                                             \nand choose your read and write databases. So here, CQRS separates reads and writes into different databases, where commands perform the creation or updating of data, and queries perform read data.  Commands are actions with some defined operations like “add the item to bucket” or “check my balance.” So, commands can be published via message brokers, which help applications process them in an async manner. Queries never modify the database. Queries always return the JSON data with DTO objects. In this way, we can isolate the Commands and Queries.  How to Sync Databases with CQRS? When we segregate read and write concerns in two different databases, the primary consideration is to sync these two data-bases properly. So, both databases should always be synced.  This can be achieved using Event Driven Architecture. As per Event Driven Architecture, when an update command is issued to the write database, it will publish an update event using message broker systems, and this will be consumed by the read database, which will pull the latest changes from the write database to keep the read database in sync.   But this option creates a consistency issue because the data would not be reflected immediately since async communication is implemented with message brokers. This will operate the principle of “eventual consistency.”  Eventual consistency is a property of distributed computing systems such that the value for a specific data item will, given enough time without up-dates, be consistent across all nodes. The read database eventually synchronizes with the write database, which will take some time to sync.  So, if we return to our read and write databases in the CQRS pattern when you start on the design, you can initially create a read database from the replicas of the write database. Also, as we have separate read and write databases, it means that both are scalable independently.     Event Sourcing  With CQRS, as reads and writes are separated, we need to keep both the database in sync as well, which is achieved using Event-Driven Architecture. But any failure in this sync or any loss of event can make data inconsistent in one of the databases, so to overcome this scenario, Event Sourcing is used. In Event Sourcing, apart from publishing the events on message broker systems, we store the events in the write database, and this event is stored in a single source of truth. In case of failure, events can be replayed, which will help keep data consistent.  Let's understand Event Sourcing with an example. Suppose there is a user’s table and a user updates their details. Usually, the updated details will override the existing values. This is generally how most applications work, i.e., they always store the entity’s current state.  But with the Event Sourcing pattern, it is changing. Instead of storing just the latest state, we store all operations in the database. The Event Sourcing pattern suggests saving all events into the database with a sequential order of data events. This events database is called an event store.  So, the event store is considered as a single source of truth for the data. After that, these event stores are converted to a read database using the materialized views. This conversion operation can also be handled by publish/subscribe patterns with events published through message broker systems. Moreover, this event list gives us the ability to replay events at a given timestamp. This allows us to build the latest status of data in case of any failure.  Let’s take the use case of a shopping cart for our e-commerce application, where we applied CQRS and Event Sourcing. As you can see in the image below, for the shopping cart, every user’s actions are recorded in the event store with appended events. All these events are combined and summarized on the read database with denormalized tables into materialized view database. Microservices Design Patterns    \n Copyright © 2023 ValueLabs. All rights reserved                                                                                                                   Page 8 of 17                                             \n By applying these patterns, we can query the latest status of the shopping cart using materialized views, which are used to create a read database. Instead of storing actual data, it stores sequential events that denote user actions, allowing us to know the history of actions the user has taken with timestamps. This enables us to retrieve the status of shopping cart data at any point in time. We can use event store write databases with Azure Cosmos DB, Cassandra, Event Store databases, etc.   SAGA   Usually, Microservices are designed around business domains, so the application is divided into multiple Microservices. However, there is a challenge, as now, any transaction will also be distributed across the services. To handle this distributed transaction scenario Saga pattern is used. \n   The Saga design pattern is used for managing data consistency in the case of distributed transactions. The Saga pattern tends to create a set of transactions that update each Microservice sequentially and publish an event to trigger the following transaction for the next Microservices. If the transaction fails at any step or service, then the Saga patterns trigger to rollback event, which basically does reverse operations in each of the Microservices.  A saga is a sequence of local transactions. Each local transaction updates the database and publishes a message or event to trigger the next local transaction in the saga. If a local transaction fails because it violates a business rule, then the saga executes a series of compensating transactions that undo the changes that were made by the preceding local transactions.   There are two options for how SAGA is implemented: • Choreography - Each local transaction publishes domain events that trigger local transactions in other services • Orchestration - An orchestrator (object) tells the participants what local transactions to execute      \nMicroservices Design Patterns    \n Copyright © 2023 ValueLabs. All rights reserved                                                                                                                   Page 9 of 17                                             \nChoreography-based saga \n  Here is the e-commerce service using events to implement choreography-based SAGA.  Choreography enables the implementation of the saga pattern by applying publish-subscribe principles. With choreography, each Microservice executes its own local transactions and publishes events to a message broker system that triggers local transactions in other Microservices.  In the above diagram, the order service receives a post request for an order at step 1 and updates the local database. Then, it publishes the ‘order created’ event on the order event channel. At the same time, the customer service is a subscriber to the order event channel and will receive an event notification. On receiving notification, customer service triggers its own local tasks, like checking if the customer has appropriate credits or not and then reserving credits for the order for which the notification was received. Once done, it will either emit a ‘credit reserved’ or ‘credit limit exceeded' event in the customer event channel, which is subscribed by the order service. This is how the entire distributed transaction works.  For fewer services, this flow works well. Still, suppose the number of services are more. In that case, it becomes complex, and apart from its own local tasks, each service has the additional responsibility of emitting & consuming events as well, which makes things furthermore complex, and that’s where the Orchestration-based saga will solve the issue.  Orchestration Based Saga  \n \nMicroservices Design Patterns    \n Copyright © 2023 ValueLabs. All rights reserved                                                                                                                   Page 10 of 17                                             \n The Orchestrator-based saga also operates on events, but the responsibility for emitting events and ensuring the atomicity of transactions shifts from the services themselves to the orchestrator. Orchestrator commands services to execute local transactions. The orchestrator acts as an agent who tells services to execute local transactions and maintains the status of the complete transaction.  In the above diagram, the order service receives a post request for placing an order at step 1. Order service will initiate a transaction here and create a saga orchestrator. This orchestrator will ask the order service to create an order in the pending state. Now the orchestrator will send a “Reserve credit” command to customer service. Customer service will reserve credit and send the status of its local transaction channel, which the orchestrator consumes. Now, if the orchestrator receives a positive status from customer service, it will ask the order service to update the order status from “pending” to “In-Progress.” But, if the credit is not reserved, then the entire transaction will be rolled-back from the order and customer service. As the orchestrator now takes care of emitting events, this type of implementation is suitable for complex workflows involving many services. New services might also be added in the future. Since the orchestrator controls every transaction, there is less chance of getting into cyclical dependency.   Use the Saga pattern when you need to: • Ensure data consistency in a distributed system without tight coupling • Roll back or compensate if one of the operations in the sequence fails   BFF (Backend for Frontend) As and when an application is designed in Microservices architecture, there may be cases where data to be displayed in the frontend, be it web or mobile, might be coming from multiple services. In such cases, it becomes the responsibility of the frontend team to transform and aggregate responses from multiple Microservices. However, there are a few challenges with this approach:  • The frontend team needs to take care of a lot of transformations and aggregations • Also, the browser or mobile app interface will use more resources for rendering the page  BFF patterns solve these challenges by introducing an intermediate layer between frontend and backend services. The BFF layer acts as a proxy that will call all backend services, aggregate and transform the response as per the needs of the frontend, and expose ready-to-consume responses that best fit the frontend. Therefore, there will be very minimal logic on the frontend. Hence, a BFF helps to streamline the data formatting process and takes up the responsibility of providing a well-focused response for the frontend to consume.           Microservices Design Patterns    \n Copyright © 2023 ValueLabs. All rights reserved                                                                                                                   Page 11 of 17                                             \nHere is an example showing BFF:  \n  Ideally, the front-end team will be responsible for managing the BFF in most cases. A single BFF is focused on a single UI and that UI only. As a result, it will help us keep our frontends simple and see a unified view of data through its backend.  Usage of BFF again depends on the needs and architectural requirements. If the application you are designing or migrating to Microservices is simple, where each service has its own UI, then BFF is not needed. However, if the application has a complex functionality that involves multiple third-party integrations and front-end screens need to show processed data from multi-ple services, BFF is the best fit. Also, if the client’s requirement is around optimal rendering of the frontend and backend, and you have complex Microservices, a BFF is suitable to use.  Furthermore, there can be multiple BFFs in the application depending on the needs and requirements of the application. In a scenario where the same backend code serves both web and mobile, where each shows data differently, there can be one BFF layer for the web and one BFF layer for mobile.  \n  To conclude, implementing these patterns should be done judiciously, and code duplication should be avoided.    \nMicroservices Design Patterns    \n Copyright © 2023 ValueLabs. All rights reserved                                                                                                                   Page 12 of 17                                             \nAPI Gateway If we need to build a large, complex Microservices-based application with multiple clients, we can use the API gateway pattern. This pattern applies to distributed systems and acts as a reverse proxy or gateway routing for requests coming in from clients. API gateway acts as communication between the client and underlying services and serves as a single-entry point to the system while abstracting the underlying architecture. It also provides cross-cutting concerns like authentication, SSL termination, and caching. \n So, as you can see, the API gateway provides a single entry point to multiple services. There are some things that we need to make sure of while using this pattern.   If there is only one node acting as an API gateway, there is a chance it will become a single point of failure. This situation needs to be handled, as if more complex logic is added to the API Gateway, it will become an anti-pattern. As the usage of this pattern grows,  it is advised to deploy multiple API Gateways for multiple services based on the use case and the number of services.  In summary, we need to be cautious about using a single API Gateway, as it should be decided based on the business context of the client applications. Ideally, it is not advised to have one API gateway for all the internal Microservices.   Strangler Pattern The Strangler Pattern is a methodology used in Microservices architecture to gradually migrate a Monolithic application to a Microservices-based architecture. This pattern is useful when the application is too large and complex to migrate simultane-ously.  The Strangler Pattern involves designing a new set of Microservices that gradually take over the functionality of the Mono-lithic application. Initially, all requests for specific functionality are divided between the traditional Monolith and the new Microservice to cater to the same functionality. Over time, as the Microservices mature, they start to handle more of the traffic, and the Monolithic application handles less. Slowly, the entire complex Monolith will be replaced by more agile and scalable Microservices.   \nMicroservices Design Patterns    \n Copyright © 2023 ValueLabs. All rights reserved                                                                                                                   Page 13 of 17                                             \n The Strangler Pattern follows these basic steps: Ø Identify the functionality of the Monolithic application around the business domain that can be converted to a                  Microservice Ø Create a new set of Microservices from scratch to handle the identified functionality Ø Then, we must configure partial traffic to the new Microservices using an API gateway or any proxy server Ø Over a period, gradually increase the traffic to the Microservices while decreasing the traffic to the Monolithic application Ø Keep monitoring the performance of the new Microservices and adjust the traffic accordingly Ø Eventually, route entire traffic to Microservices and bring down the entire Monolithic service  Circuit Breaker The Circuit Breaker Pattern is a technique used in Microservices architecture to improve the resilience and reliability of dis-tributed systems. The basic idea behind the circuit breaker is very simple. You wrap a function call in a circuit breaker object, which monitors it for failures. Once the failures reach a certain threshold, the circuit breaker trips and all further calls to the circuit breaker return an error, with any further calls skipped. The Circuit Breaker can also be configured to handle failures in different ways, such as returning a default value or a cached response.  \nMicroservices Design Patterns    \n Copyright © 2023 ValueLabs. All rights reserved                                                                                                                   Page 14 of 17                                             \n                           Here are the basic steps involved in the Circuit Breaker Pattern: Ø Identify the Microservices that need to communicate with each other Ø Add a Circuit Breaker between the Microservices Ø Add configurations to monitor the status of the Microservices and detect failures Ø If a Microservice fails to respond, the Circuit Breaker can prevent further requests to the Microservice and handle the fail-ure in an appropriate way Ø In some cases, the default response is also configured, which will help to handle failures gracefully.  Externalized Configuration In Microservices architectures, systems are divided into multiple Microservices, each one running in its own container. Each process can be deployed and independently scaled, which means there may be many instances of the same microservice running at a specific time.  Let’s say we want to modify the configurations for a microservice that has been replicated almost a hundred times.  If the configuration for this microservice is packaged with the service itself, we need to do deployment again for each in-stance running. This can lead to an inconsistent state as there are high chances that a few instances still need to upgrade to a new build and are still running on old configurations. Therefore, it is advised that services share the external configuration.  \nMicroservices Design Patterns    \n Copyright © 2023 ValueLabs. All rights reserved                                                                                                                   Page 15 of 17                                             \n It works by keeping the configurations in an external store, such as a database, file system, or environment variables. When the Microservice is deployed and started, it loads the configuration from the external source. At runtime, if configuration changes occur, those are generally reloaded by Microservices without any new deployments.  Consumer-Driven Contract Tracing This pattern can be referred to as the test-driven development pattern for the development of Microservices. The pattern suggests taking a design-first approach where negotiation on the expected response between provider and consumer de-cides the best possible outcome. The developers of a consumer service write “contracts” specifying the responses they ex-pect from the requests made to a service provider. It is “consumer-driven” because the consumer’s developers drive the writing of the contract and lead the negotiations with the provider’s developers.   The contract is typically a JSON or XML file that the developers and their services (consumer and provider) can access. One of the main benefits of consumer-driven contract testing is that it allows collaboration from the start between service providers and consumers. It also helps service developers to understand the requirements and expectations of consumers in a proac-tive manner.   Another benefit of CDCT is that it helps to prevent integration issues between services. By defining the contract up front, both the consumer and provider can ensure they are on the same page, saving many testing cycles as well.  Here are the steps to implement CDCT: 1. Define the contract: The consumer of a service should define the contract that the service provider must follow. This should include the expected input and output of the service, as well as any other requirements that the consumer has. 2. Implement the contract: The service provider should implement the contract and ensure that their service meets the re-quirements.  3. Run tests: The service provider should then run test cases to ensure the service meets the requirements. This can include both unit tests and integration tests.  4. Publish the contract: Once the service provider has implemented the contract and passed their tests, they should             publish the contract to the consumer.  \nMicroservices Design Patterns    \n Copyright © 2023 ValueLabs. All rights reserved                                                                                                                   Page 16 of 17                                             \n5. Verify compatibility: Consumers can then verify that they got what was agreed upon. \nConclusion With the increase in the usage of Microservices-based architecture, complexities arise in managing scalability or handling distributed services' transactions. However, a set of defined patterns, which have been tested repeatedly, give solutions to problems that are very common for Microservice-based architectures. Knowing each pattern provides good insight into how Microservices architecture handles performance, scalability, agility, and maintainability. We hope that the few patterns de-scribed above provide good insight for you.           \nMicroservices Design Patterns    \n Copyright © 2023 ValueLabs. All rights reserved                                                                                                                   Page 17 of 17                                             \nAbout the Author  \n  Nishant Malhotra is working as an Architect - Digital Consulting at ValueLabs. He has over 13 years of experience in deliver-ing enterprise solutions in e-commerce, m-commerce, workflow, and web-based areas. He is also well-versed in executing solutions for complex business problems involving large-scale data handling, real-time analytics, and reporting solutions. His other areas of expertise include defining architecture, designing, and technical coding solutions using Java/J2EE and cloud infrastructure.  Nishant can be reached at nishant.malhotra@valuelabs.com       \n"]
2025-02-05 14:14:31,511 - INFO - Text extracted from pdf and converted into chunks
2025-02-05 14:14:33,768 - INFO - (Collection(name=Microservices-Design-Patterns.pdf), 'Microservices-Design-Patterns.pdf')
2025-02-05 14:14:33,775 - ERROR - Failed to get relevent passage, Please change the query
2025-02-05 14:14:33,775 - ERROR - Failed to perform db operations 'tuple' object has no attribute 'query'
2025-02-05 14:18:23,407 - INFO - db configuration is localhost, 8000
2025-02-05 14:18:23,571 - INFO - successfully connected to chromadb
2025-02-05 14:18:31,399 - INFO - Text extracted from pdf and converted into chunks
2025-02-05 14:18:33,612 - INFO - (Collection(name=Microservices-Design-Patterns.pdf), 'Microservices-Design-Patterns.pdf')
2025-02-05 14:18:33,620 - INFO - Collection(name=Microservices-Design-Patterns.pdf)
2025-02-05 14:18:33,621 - ERROR - Failed to get relevent passage, Please change the query
2025-02-05 14:18:33,622 - ERROR - Failed to perform db operations 'tuple' object has no attribute 'query'
2025-02-05 14:25:49,061 - INFO - db configuration is localhost, 8000
2025-02-05 14:25:49,218 - INFO - successfully connected to chromadb
2025-02-05 14:26:03,539 - INFO - db configuration is localhost, 8000
2025-02-05 14:26:03,732 - INFO - successfully connected to chromadb
2025-02-05 14:26:27,698 - INFO - Text extracted from pdf and converted into chunks
2025-02-05 14:26:28,587 - INFO - Microservices-Design-Patterns.pdf
2025-02-05 14:26:28,593 - INFO - Collection(name=Microservices-Design-Patterns.pdf)
2025-02-05 14:26:30,401 - ERROR - Failed to make rag prompt 'list' object has no attribute 'replace'
2025-02-05 14:26:30,401 - ERROR - Failed to get relevent passage, Please change the query
2025-02-05 14:26:30,401 - ERROR - Failed to perform db operations 'list' object has no attribute 'replace'
2025-02-05 14:27:58,332 - INFO - db configuration is localhost, 8000
2025-02-05 14:27:58,515 - INFO - successfully connected to chromadb
2025-02-05 14:28:01,436 - INFO - Text extracted from pdf and converted into chunks
2025-02-05 14:28:03,665 - INFO - Microservices-Design-Patterns.pdf
2025-02-05 14:28:03,671 - INFO - Collection(name=Microservices-Design-Patterns.pdf)
2025-02-05 14:28:05,487 - ERROR - Failed to make rag prompt 'list' object has no attribute 'replace'
2025-02-05 14:28:05,487 - ERROR - Failed to get relevent passage, Please change the query
2025-02-05 14:28:05,487 - ERROR - Failed to perform db operations 'list' object has no attribute 'replace'
2025-02-05 14:29:37,891 - INFO - db configuration is localhost, 8000
2025-02-05 14:29:38,067 - INFO - successfully connected to chromadb
2025-02-05 14:29:43,004 - INFO - Text extracted from pdf and converted into chunks
2025-02-05 14:29:43,973 - INFO - Microservices-Design-Patterns.pdf
2025-02-05 14:29:43,982 - INFO - Collection(name=Microservices-Design-Patterns.pdf)
2025-02-05 14:31:56,523 - INFO - Text extracted from pdf and converted into chunks
2025-02-05 14:31:58,758 - INFO - Microservices-Design-Patterns.pdf
2025-02-05 14:31:58,766 - INFO - Collection(name=Microservices-Design-Patterns.pdf)
2025-02-05 14:33:18,483 - INFO - Text extracted from pdf and converted into chunks
2025-02-05 14:33:20,659 - INFO - Microservices-Design-Patterns.pdf
2025-02-05 14:33:20,666 - INFO - Collection(name=Microservices-Design-Patterns.pdf)
2025-02-05 14:35:46,035 - INFO - db configuration is localhost, 8000
2025-02-05 14:35:46,185 - INFO - successfully connected to chromadb
2025-02-05 14:35:57,297 - INFO - Text extracted from pdf and converted into chunks
2025-02-05 14:35:59,516 - INFO - Microservices-Design-Patterns.pdf
2025-02-05 14:35:59,530 - INFO - Collection(name=Microservices-Design-Patterns.pdf)
2025-12-11 18:17:17,835 - INFO - db configuration is localhost, 8000
2025-12-11 18:17:18,541 - INFO - successfully connected to chromadb
2025-12-11 18:18:21,892 - INFO - db configuration is localhost, 8000
2025-12-11 18:18:22,096 - INFO - successfully connected to chromadb
2025-12-11 18:18:44,464 - INFO - db configuration is localhost, 8000
2025-12-11 18:18:44,652 - INFO - successfully connected to chromadb
2025-12-11 18:24:56,048 - INFO - Text extracted from pdf and converted into chunks
2025-12-11 18:24:56,087 - ERROR - Failed to perform db operations Collection [Kubernetes-for-Beginners.pdf] does not exist
2025-12-11 18:26:31,822 - INFO - Text extracted from pdf and converted into chunks
2025-12-11 18:26:31,827 - ERROR - Failed to perform db operations Collection [Kubernetes-for-Beginners.pdf] does not exist
2025-12-13 18:02:12,559 - INFO - db configuration is localhost, 8000
2025-12-13 18:02:12,990 - INFO - successfully connected to chromadb
2025-12-13 18:02:32,142 - INFO - Text extracted from pdf and converted into chunks
2025-12-13 18:02:32,185 - ERROR - Failed to perform db operations Collection [Kubernetes-for-Beginners.pdf] does not exist
2025-12-13 18:04:27,573 - INFO - db configuration is localhost, 8000
2025-12-13 18:04:27,790 - INFO - successfully connected to chromadb
2025-12-13 18:04:40,662 - INFO - Text extracted from pdf and converted into chunks
2025-12-13 18:04:40,979 - ERROR - Failed to perform db operations 401 API keys are not supported by this API. Expected OAuth2 access token or other authentication credentials that assert a principal. See https://cloud.google.com/docs/authentication [reason: "CREDENTIALS_MISSING"
domain: "googleapis.com"
metadata {
  key: "service"
  value: "generativelanguage.googleapis.com"
}
metadata {
  key: "method"
  value: "google.ai.generativelanguage.v1beta.GenerativeService.BatchEmbedContents"
}
]
2025-12-14 06:45:22,426 - INFO - db configuration is localhost, 8000
2025-12-14 06:45:23,201 - INFO - successfully connected to chromadb
2025-12-14 06:53:02,890 - INFO - Text extracted from pdf and converted into chunks
2025-12-14 06:53:02,936 - ERROR - Failed to perform db operations Validation error: name: Expected a name containing 3-512 characters from [a-zA-Z0-9._-], starting and ending with a character in [a-zA-Z0-9]. Got: Power+BI+for+Business+Intelligence.pdf
2025-12-16 17:09:00,513 - INFO - db configuration is localhost, 8000
2025-12-16 17:09:00,998 - INFO - successfully connected to chromadb
2025-12-16 18:08:12,393 - INFO - db configuration is localhost, 8000
2025-12-16 18:08:12,685 - INFO - successfully connected to chromadb
2025-12-16 18:13:06,946 - INFO - db configuration is localhost, 8000
2025-12-16 18:13:07,351 - INFO - successfully connected to chromadb
2025-12-16 18:14:57,024 - INFO - Text extracted from pdf and converted into chunks
2025-12-16 18:14:57,401 - ERROR - Failed to perform db operations 401 API keys are not supported by this API. Expected OAuth2 access token or other authentication credentials that assert a principal. See https://cloud.google.com/docs/authentication [reason: "CREDENTIALS_MISSING"
domain: "googleapis.com"
metadata {
  key: "service"
  value: "generativelanguage.googleapis.com"
}
metadata {
  key: "method"
  value: "google.ai.generativelanguage.v1beta.GenerativeService.BatchEmbedContents"
}
]
2025-12-18 05:38:44,866 - INFO - db configuration is localhost, 8000
2025-12-18 05:38:45,417 - INFO - successfully connected to chromadb
2025-12-18 05:39:36,776 - INFO - Text extracted from pdf and converted into chunks
2025-12-18 05:39:36,789 - ERROR - Failed to perform db operations Validation error: name: Expected a name containing 3-512 characters from [a-zA-Z0-9._-], starting and ending with a character in [a-zA-Z0-9]. Got: Kiran Kumar T - Business_Data_Analyst.pdf
2025-12-18 05:41:31,788 - INFO - Text extracted from pdf and converted into chunks
2025-12-18 05:41:31,861 - ERROR - Failed to perform db operations Collection [PowerBI_Embedded_Prerequisite_Checklist.pdf] already exists
2025-12-18 05:45:49,992 - INFO - Text extracted from pdf and converted into chunks
2025-12-18 05:45:50,171 - ERROR - Failed to perform db operations 401 API keys are not supported by this API. Expected OAuth2 access token or other authentication credentials that assert a principal. See https://cloud.google.com/docs/authentication [reason: "CREDENTIALS_MISSING"
domain: "googleapis.com"
metadata {
  key: "service"
  value: "generativelanguage.googleapis.com"
}
metadata {
  key: "method"
  value: "google.ai.generativelanguage.v1beta.GenerativeService.BatchEmbedContents"
}
]
2025-12-18 05:54:50,342 - INFO - db configuration is localhost, 8000
2025-12-18 05:54:50,592 - INFO - successfully connected to chromadb
2025-12-18 05:56:06,818 - INFO - Text extracted from pdf and converted into chunks
2025-12-18 05:56:06,920 - ERROR - Failed to perform db operations 401 API keys are not supported by this API. Expected OAuth2 access token or other authentication credentials that assert a principal. See https://cloud.google.com/docs/authentication [reason: "CREDENTIALS_MISSING"
domain: "googleapis.com"
metadata {
  key: "service"
  value: "generativelanguage.googleapis.com"
}
metadata {
  key: "method"
  value: "google.ai.generativelanguage.v1beta.GenerativeService.BatchEmbedContents"
}
]
2025-12-18 05:56:10,998 - INFO - Text extracted from pdf and converted into chunks
2025-12-18 05:56:11,003 - ERROR - Failed to perform db operations Collection [Sagility_BI_Manager_Interview_Prep_Kiran.pdf] already exists
2025-12-19 17:20:23,729 - INFO - db configuration is localhost, 8000
2025-12-19 17:20:24,110 - INFO - successfully connected to chromadb
2025-12-19 17:21:07,794 - INFO - Text extracted from pdf and converted into chunks
2025-12-19 17:21:07,944 - ERROR - Failed to perform db operations 401 API keys are not supported by this API. Expected OAuth2 access token or other authentication credentials that assert a principal. See https://cloud.google.com/docs/authentication [reason: "CREDENTIALS_MISSING"
domain: "googleapis.com"
metadata {
  key: "service"
  value: "generativelanguage.googleapis.com"
}
metadata {
  key: "method"
  value: "google.ai.generativelanguage.v1beta.GenerativeService.BatchEmbedContents"
}
]
2025-12-19 17:24:13,125 - INFO - db configuration is localhost, 8000
2025-12-19 17:24:13,351 - INFO - successfully connected to chromadb
2025-12-19 17:24:26,458 - INFO - ['MUMSHAD MANNAMBETH\nfor beginners\nmumshad mannambeth\nHello and welcome to this course on Kubernetes for the Absolute Beginners. My \nname is Mumshad Mannambeth and I will be your instructor for this course. \n1MUMSHAD MANNAMBETH\nWho am I?\nSo about me, I am a Solutions Architect and I work on Cloud Automation and DevOps \nPractice. I have authored several Best Seller and Top Rated courses on Udemy on \ntopics like Docker and Ansible. \n2MUMSHAD MANNAMBETH\nCourse Structure\nLecture Demos Quiz Coding Exercises\nTips & Tricks Assignment Q&A\nLet’s take a look at the structure of this course. This course contains a series of \nlectures on Kubernetes, we will be discussing the basic concepts of kubernetes here. \nThese are followed by some demos on setting up a test environment and getting \nstarted with kubernetes.  \nWe have optional quizzes that test your knowledge after each lecture. These are \ncompletely optional, you don’t have to pass these to continue the course. \nWe then have coding exercises that will help you learn and practice developing YAML \nfiles which will enable you to develop kubernetes configuration files yourself. \nThese configuration files may be a bit tricky for beginners so I will also provide some \ntips and tricks that will help you remember the format. This plus the coding exercises \nwill give you enough confidence to develop your own configuration files. We will in \nfact spend some EXTRA time on these configuration files, because in order to work \nwith kubernetes you MUST have a good understanding around it.\nAt the end you will be given an assignment to test your knowledge on the topic. The \nassignment will help you gain exposure in working on a real life task with Kubernetes.\n3And as always if you have any questions, you may reach out directly to me through \nthe Q&A section.\n3MUMSHAD MANNAMBETH\nWho is this for?\nDevelopers System Admins Managers\nSo  who is this course for? THIS is for the Absolute beginner. You don’t need to have \nany prior experience working with Kubernetes. We will walk through all the basic \nconcepts required for you to understand Kubernetes. This is for those who are new to \ncontainers and container orchestration technology. You could be a developer trying to \nunderstand microservices, containers and orchestration technology - looking to gain \nsome hands on experience in developing kubernetes solutions. You could be a system \nadmin looking to scale up your knowledge in container clusters and trying to get \nsome hands on experience in setting up such clusters. You could be a Project \nManager looking to gain only a basic understanding of the underlying concepts. This \nis for any beginner starting his Kubernetes journey.\n4MUMSHAD MANNAMBETH\nHow to attend this course?\nLectures\nDemos\nQuizzes\nCoding Exercises\nLab Environment AWS|GCPLocal Play-with-k8s\nSo that brings us to the question on how you should take up this course. Depending \non your interest and the time and resources you have at hand, you may chose to skip \ncertain sections of the course. If you are already familiar with the basic concepts such \nas docker containers and YAML files feel free to skip those sections. \nIn this course, we will see different ways of setting up kubernetes - on your local\nmachine or on various cloud platforms such as Google Cloud or Amazon etc. If you \nare in a hurry or do not have sufficient resources to setup your own instances, we will \nalso see option available to spin up a kubernetes cluster in a matter of seconds on the \ncloud. \nAlso remember this course is for absolute beginners. You will see us discussing real \nbasic stuff, we will focus on each concept and try and understand it simply. When we \ndo that if you have doubts in other areas, hold on to them. Because we might discuss \nthem during a later lecture. Also note that from time to time you might hear me \nrepeat certain sentences or phrases, and we might be recapping what we learned in \nprevious lectures more often. So bear me with, because I do this to ensure that the \nkey concepts stick to your mind and recapping more often will help you relate newer \nconcepts to older ones better.\n5MUMSHAD MANNAMBETH\nObjectives\nKubernetes Overview\nContainers – Docker\nContainer Orchestration?\nDemo - Setup Kubernetes\nKubernetes Concepts – PODs | ReplicaSets | Deployment | Services\nNetworking in Kubernetes\nKubernetes Management - Kubectl\nKubernetes Definition Files - YAML\nKubernetes on Cloud – AWS/GCP\nMoving on the topics we will be covering. In this course we will go through the basics \nof Kubernetes, we will try to understand what containers are and what container \norchestration is. We will see different ways of setting up and getting started with \nKubernetes. We will go through various concepts such as PODs, ReplicaSets, \nDeployments and Services. We will understand the basics of Networking in \nkubernetes. We will also spend some time working with kubectl command line utility \nand developing kubernetes YAML definition files. And finally we will see how to \ndeploy a microservices application on a public cloud platform like Google Cloud.\nAs always feel free to go through this course at your own pace. There may be sections \nin the course that you may be familiar with already, and so feel free to skip them. \nLet’s get started and I will see you in the first module.\n6MUMSHAD MANNAMBETH\nkubernetes or K8s\nContainer + Orchestration\nIn this lecture we will go through an overview of Kubernetes. \nKubernetes also known as K8s was built by Google based on their experience running \ncontainers in production. It is now an open-source project and is arguably one of the \nbest and most popular container orchestration technologies out there.  In this lecture \nwe will try to understand Kubernetes at a high level.  \nTo understand Kubernetes, we must first understand two things – Container and \nOrchestration. Once we get familiarized with both of these terms we would be in a \nposition to understand what kubernetes is capable of. We will start looking at each of \nthese next. \n7MUMSHAD MANNAMBETH\nContainers\nmumshad mannambeth\nWe are now going to look at what containers are, specifically we will look at the most \npopular container technology out there – Docker. If you are familiar with Docker \nalready, feel free to skip this lecture and move over to the next.\n8MUMSHAD MANNAMBETH\nWhy do you need containers?\nOS\nWeb Server Database Messaging Orchestration\nLibraries Dependencies\nCompatibility/Dependency\nLong setup time\nDifferent Dev/Test/Prod \nenvironments\nHardware Infrastructure\nThe Matrix from Hell !!\nLet me start by sharing how I got introduced to Docker. In one of my previous \nprojects, I had this requirement to setup an end-to-end stack including various \ndifferent technologies like a Web Server using NodeJS and a database such as \nMongoDB/CouchDB, messaging system like Redis and an orchestration tool like \nAnsible. We had a lot of issues developing this application with all these different \ncomponents. First, their compatibility with the underlying OS. We had to ensure that \nall these different services were compatible with the version of the OS we were \nplanning to use. There have been times when certain version of these services were \nnot compatible with the OS, and we have had to go back and look for another OS that \nwas compatible with all of these different services. \nSecondly, we had to check the compatibility between these services and the libraries \nand dependencies on the OS. We have had issues were one service requires one \nversion of a dependent library whereas another service required another version. \nThe architecture of our application changed over time, we have had to upgrade to \nnewer versions of these components, or change the database etc and everytime\nsomething changed we had to go through the same process of checking compatibility \nbetween these various components and the underlying infrastructure. This \n9compatibility matrix issue is usually referred to as the matrix from hell. \nNext, everytime we had a new developer on board, we found it really difficult to \nsetup a new environment. The new developers had to follow a large set of \ninstructions and run 100s of commands to finally setup their environments. They had \nto make sure they were using the right Operating System, the right versions of each \nof these components and each developer had to set all that up by himself each time. \nWe also had different development test and production environments. One \ndeveloper may be comfortable using one OS, and the others may be using another \none and so we couldn’t gurantee the application that we were building would run the \nsame way in different environments.  And So all of this made our life in developing, \nbuilding and shipping the application really difficult.\n9MUMSHAD MANNAMBETH\n?\nWhat can it do?\nDocker\nWeb Server Database Messaging Orchestration\nOS\nLibs Deps Libs Deps Libs Deps Libs Deps\nContainerize Applications\nRun each service with its own \ndependencies in separate containers\nContainer Container Container Container\nHardware Infrastructure\nSo I needed something that could help us with the compatibility issue. And \nsomething that will allow us to modify or change these components without affecting \nthe other components and even modify the underlying operating systems as \nrequired. And that search landed me on Docker. With Docker I was able to run each \ncomponent in a separate container – with its own libraries and its own dependencies. \nAll on the same VM and the OS, but within separate environments or containers.  We \njust had to build the docker configuration once, and all our developers could now get \nstarted with a simple “docker run” command. Irrespective of what underlying OS they \nrun, all they needed to do was to make sure they had Docker installed on their \nsystems.\n10MUMSHAD MANNAMBETH\nWhat are containers?\nProcesses\nNetwork\nMounts\nDocker\nOS Kernel\nProcesses\nNetwork\nMounts\nProcesses\nNetwork\nMounts\nProcesses\nNetwork\nMounts\nSo what are containers? Containers are completely isolated environments, as in they \ncan have their own processes or services, their own network interfaces, their own \nmounts, just like Virtual machines, except that they all share the same OS kernel. We \nwill look at what that means in a bit. But its also important to note that containers \nare not new with Docker. Containers have existed for about 10 years now and some \nof the different types of containers are LXC, LXD , LXCFS etc. Docker utilizes LXC \ncontainers. Setting up these container environments is hard as they are very low level \nand that is were Docker offers a high-level tool with several powerful functionalities \nmaking it really easy for end users like us.\n11MUMSHAD MANNAMBETH\nOperating system\nOS\nOS Kernel\nSoftware Software Software Software\nTo understand how Docker works let us revisit some basics concepts of Operating \nSystems first. If you look at operating systems like Ubuntu, Fedora, Suse or Centos –\nthey all consist of two things. An OS Kernel and a set of software. The OS Kernel is \nresponsible for interacting with the underlying hardware. While the OS kernel \nremains the same– which is Linux in this case, it’s the software above it that make \nthese Operating Systems different. This software may consist of a different User \nInterface, drivers, compilers, File managers, developer tools etc.  SO you have a \ncommon Linux Kernel shared across all Oses and some custom softwares that \ndifferentiate Operating systems from each other. \n12MUMSHAD MANNAMBETH\nSharing the kernel\nOS - Ubuntu\nDocker\nSoftware\n Software\n Software\n Software\nWe said earlier that Docker containers share the underlying kernel. What does that \nactually mean – sharing the kernel? Let’s say we have a system with an Ubuntu OS \nwith Docker installed on it. Docker can run any flavor of OS on top of it as long as they \nare all based on the same kernel – in this case Linux. If the underlying OS is Ubuntu, \ndocker can run a container based on another distribution like debian, fedora, suse or \ncentos. Each docker container only has the additional software, that we just talked \nabout in the previous slide, that makes these operating systems different and docker\nutilizes the underlying kernel of the Docker host which works with all Oses above.\nSo what is an OS that do not share the same kernel as these? Windows ! And so you \nwont be able to run a windows based container on a Docker host with Linux OS on it. \nFor that you would require docker on a windows server.\nYou might ask isn’t that a disadvantage then? Not being able to run another kernel on \nthe OS? The answer is No! Because unlike hypervisors, Docker is not meant to \nvirtualize and run different Operating systems and kernels on the same hardware. The \nmain purpose of Docker is to containerize applications and to ship them and run \nthem. \n13MUMSHAD MANNAMBETH\nContainers vs Virtual Machines\nDocker\nOS\nLibs Deps\nHardware Infrastructure\nContainer Container\nLibs Deps\nHypervisor\nOS\nLibs Deps\nHardware Infrastructure\nVirtual Machine Virtual Machine\nLibs Deps Application Application\nOS OS\nApplication Application\nUtilization UtilizationSize Size\nGB\nMB\nBoot up Boot up\n✓\n✓\n✓\n✓\n✓\n✓\n✓\n✓\nSo that brings us to the differences between virtual machines and containers. \nSomething that we tend to do, especially those from a Virtualization.\nAs you can see on the right, in case of Docker, we have the underlying hardware \ninfrastructure, then the OS, and Docker installed on the OS. Docker then manages the \ncontainers that run with libraries and dependencies alone. In case of a Virtual \nMachine,  we have the OS on the underlying hardware, then the Hypervisor like a ESX \nor virtualization of some kind and then the virtual machines. As you can see each \nvirtual machine has its own OS inside it, then the dependencies and then the \napplication.\nThis overhead causes higher utilization of underlying resources as there are multiple \nvirtual operating systems and kernel running. The virtual machines also consume \nhigher disk space as each VM is heavy and is usually in Giga Bytes in size, wereas\ndocker containers are lightweight and are usually in Mega Bytes in size.\nThis allows docker containers to boot up faster, usually in a matter of seconds \nwhereas VMs we know takes minutes to boot up as it needs to bootup the entire OS. \n14It is also important to note that, Docker has less isolation as more resources are \nshared between containers like the kernel etc. Whereas VMs have complete isolation \nfrom each other. Since VMs don’t rely on the underlying OS or kernel, you can run \ndifferent types of OS such as linux based or windows based on the same hypervisor.\nSo these are some differences between the two.\n14MUMSHAD MANNAMBETH\nHow is it done?\ndocker run ansible\ndocker run mongodb\ndocker run redis\ndocker run nodejs\ndocker run nodejs\ndocker run nodejs\nPublic Docker registry - dockerhub\nSO how is it done? There are a lot of containerized versions of applications readily \navailable as of today. So most organizations have their products containerizedand \navailable in a public docker registry called dockerhub/or docker store already. For \nexample you can find images of most common operating systems, databases and \nother services and tools. Once you identify the images you need and you install \nDocker on your host.. \nbringing up an application stack, is as easy as running a docker run command with \nthe name of the image. In this case running a docker run ansible command will run an \ninstance of ansible on the docker host. Similarly run an instance of mongodb, redis\nand nodejs using the docker run command. And  then when you run nodejs just point \nto the location of the code repository on the host.  If we need to run multiple \ninstances of the web service, simply add as many instances as you need, and \nconfigure a load balancer of some kind in the front. In case one of the instances was \nto fail, simply destroy that instance and launch a new instance. There are other \nsolutions available for handling such cases, that we will look at later during this \ncourse.\n15MUMSHAD MANNAMBETH\nContainer vs image\nDocker Image\nDocker Container #1\nDocker Container #2\nDocker Container #3\nPackage\nTemplate\nPlan\nWe have been talking about images and containers. Let’s understand the difference\nbetween the two. \nAn image is a package or a template, just like a VM template that you might have \nworked with in the virtualization world. It is used to create one or more containers. \nContainers are running instances off images that are isolated and have their own \nenvironments and set of processes\nAs we have seen before a lot of products have been dockerized already. In case you \ncannot find what you are looking for you could create an image yourself and push it \nto the Docker hub repository making it available for public.\n16MUMSHAD MANNAMBETH\nApp.war\nDockerFile\nContainer Advantage\nDeveloper Operations\nApp.war\nGuide\nDocker Image\nIf you look at it, traditionally developers developed applications. Then they hand it \nover to Ops team to deploy and manage it in production environments. They do that \nby providing a set of instructions such as information about how the hosts must be \nsetup, what pre-requisites are to be installed on the host and how the dependencies \nare to be configured etc. Since the Ops team did not develop the application on their \nown, they struggle with setting it up. When they hit an issue, they work with the \ndevelopers to resolve it. \nWith Docker, a major portion of work involved in setting up the infrastructure is now \nin the hands of the developers in the form of a Docker file. The guide that the \ndevelopers built previously to setup the infrastructure can now easily put together \ninto a Dockerfile to create an image for their applications. This image can now run on \nany container platform and is guaranteed to run the same way everywhere. So the \nOps team now can simply use the image to deploy the application. Since the image \nwas already working when the developer built it and operations are not modifying it, \nit continues to work the same when deployed in production.\n17MUMSHAD MANNAMBETH\nMore about containers\nTo learn more about containers, checkout my other courses - Docker for the Absolute \nBeginners and Docker Swarm were you can learn and practice docker commands and \ncreate docker files. That’s the end of this lecture on Containers and Docker.  See you \nin the next lecture.\n18MUMSHAD MANNAMBETH\nContainer \nOrchestration\nmumshad mannambeth\nIn this lecture we will talk about Container Orchestration.\n19MUMSHAD MANNAMBETH\nContainer Orchestration\nHost Host Host Host\nOrchestration\nSo we learned about containers and we now have our application packaged into a \ndocker container.  But what next? How do you run it in production? What if your \napplication relies on other containers such as database or messaging services or \nother backend services? What if the number of users increase and you need to scale \nyour application? You would also like to scale down when the load decreases. \nTo enable these functionalities you need an underlying platform with a set of \nresources. The platform needs to orchestrate the connectivity between the \ncontainers and automatically scale up or down based on the load. This whole process \nof automatically deploying and managing containers is known as Container \nOrchestration. \n20MUMSHAD MANNAMBETH\nOrchestration Technologies\nDocker Swarm\nKubernetes is thus a container orchestration technology. There are multiple such \ntechnologies available today – Docker has its own tool called Docker Swarm. \nKubernetes from Google and Mesos from Apache. While Docker Swarm is really easy \nto setup and get started, it lacks some of the advanced autoscaling features required \nfor complex applications. Mesos on the other hand is quite difficult to setup and get \nstarted, but supports many advanced features. Kubernetes - arguably the most \npopular of it all – is a bit difficult to setup and get started but provides a lot of options \nto customize deployments and supports deployment of complex architectures. \nKubernetes is now supported on all public cloud service providers like GCP , Azure and \nAWS and the kubernetes project is one of the top ranked projects in Github.\n21MUMSHAD MANNAMBETH\nKubernetes Advantage\nKubernetes\nWeb\nBackend\nOrchestration\nKubernetes\n Kubernetes\n Kubernetes\nWeb Web Web\nBackend Backend Backend\nWeb Web Web Web\nKubernetes\n Kubernetes\nWeb\nBackend\nThere are various advantages of container orchestration. Your application is now \nhighly available as hardware failures do not bring your application down because you \nhave multiple instances of your application running on different nodes. The user \ntraffic is load balanced across the various containers. When demand increases, \ndeploy more instances of the application seamlessly and within a matter of second \nand we have the ability to do that at a service level. When we run out of hardware \nresources, scale the number of nodes up/down without having to take down the \napplication. And do all of these easily with a set of declarative object configuration \nfiles. \n22MUMSHAD MANNAMBETH\nAnd that is kubernetes..\nAnd THAT IS Kubernetes. It is a container Orchestration technology used to \norchestrate the deployment and management of 100s and 1000s of containers in a \nclustered environment. Don’t worry if you didn’t get all of what was just said, in the \nupcoming lectures we will take a deeper look at the architecture and various \nconcepts surrounding kubernetes. That is all for this lecture, thank you for listening  \nand I will see you in the next lecture.\n23MUMSHAD MANNAMBETH\nArchitecture\nmumshad mannambeth\nBefore we head into setting up a kubernetes cluster, it is important to understand \nsome of the basic concepts. This is to make sense of the terms that we will come \nacross while setting up a kubernetes cluster.\n24MUMSHAD MANNAMBETH\nNode\nNodes\n(Minions)\nLet us start with Nodes. A node is a machine – physical or virtual – on which \nkubernetes is installed. A node is a worker machine and this is were containers will be \nlaunched by kubernetes. \nIt was also known as Minions in the past. So you might here these terms used inter \nchangeably.\nBut what if the node on which our application is running fails? Well, obviously our \napplication goes down. So you need to have more than one nodes. \n25MUMSHAD MANNAMBETH\nCluster\nNode\nNode\n Node\nA cluster is a set of nodes grouped together. This way even if one node fails you have \nyour application still accessible from the other nodes. Moreover having multiple \nnodes helps in sharing load as well.\n26MUMSHAD MANNAMBETH\nNode\nMaster\nNode\n Node\nMaster\nNow we have a cluster, but who is responsible for managing the cluster? Were is the \ninformation about the members of the cluster stored? How are the nodes \nmonitored? When a node fails how do you move the workload of the failed node to \nanother worker node? That’s were the Master comes in. The master is another node \nwith Kubernetes installed in it, and is configured as a Master.   The master watches \nover the nodes in the cluster and is responsible for the actual orchestration of \ncontainers on the worker nodes. \n27MUMSHAD MANNAMBETH\nAPI \nServer\netcd\nkubelet\nContainer \nRuntime\nController\nScheduler\nKube\nComponents\nKey-value store\nWhen you install Kubernetes on a System, you are actually installing the following \ncomponents. An API Server. An ETCD service. A kubelet service. A Container Runtime, \nControllers and Schedulers.\nThe API server acts as the front-end for kubernetes. The users, management devices, \nCommand line interfaces all talk to the API server to interact with the kubernetes \ncluster.\nNext is the ETCD key store. ETCD is a distributed reliable key-value store used by \nkubernetes to store all data used to manage the cluster. Think of it this way, when you \nhave multiple nodes and multiple masters in your cluster, etcd stores all that \ninformation on all the nodes in the cluster in a distributed manner. ETCD is \nresponsible for implementing locks within the cluster to ensure there are no conflicts \nbetween the Masters. \nThe scheduler is responsible for distributing work or containers across multiple \nnodes.  It looks for newly created containers and assigns them to Nodes. \n28The controllers are the brain behind orchestration. They are responsible for noticing \nand responding when nodes, containers or endpoints goes down. The controllers \nmakes decisions to bring up new containers in such cases.\nThe container runtime is the underlying software that is used to run containers. In our \ncase it happens to be Docker. \nAnd finally kubelet is the agent that runs on each node in the cluster. The agent is \nresponsible for making sure that the containers are running on the nodes as \nexpected.\n28MUMSHAD MANNAMBETH\nMaster vs Worker Nodes\nWorker Node\nMaster\n</> kube-apiserver </> kubelet\nContainer Runtime\netcd\ncontroller\nscheduler\nCRI-O\nSo far we saw two types of servers – Master and Worker and a set of components \nthat make up Kubernetes. But how are these components distributed across different \ntypes of servers. In other words, how does one server become a master and the \nother slave? \nThe worker node (or minion) as it is also known, is were the containers are hosted. \nFor example Docker containers, and to run docker containers on a system, we need a \ncontainer runtime installed. And that’s were the container runtime falls. In this case it \nhappens to be Docker. This doesn’t HAVE to be docker, there are other container \nruntime alternatives available such as Rocket or CRIO. But throughout this course we \nare going to use Docker as our container runtime.\nThe master server has the kube-apiserver and that is what makes it a master. \nSimilarly the worker nodes have the kubelet agent that is responsible for interacting \nwith the master to provide health information of the worker node and carry out \nactions requested by the master on the worker nodes.  \n29All the information gathered are stored in a key-value store on the Master. The key \nvalue store is based on the popular etcd framework as we just discussed.\nThe master also has the controller manager and the scheduler.\nThere are other components as well, but we will stop there for now. The reason we \nwent through this is to understand what components constitute the master and \nworker nodes. This will help us install and configure the right components on \ndifferent systems when we setup our infrastructure. \n29MUMSHAD MANNAMBETH\nkubectl\nkubectl run hello-minikube\nkubectl cluster-info\nkubectl get nodes\nAnd finally, we also need to learn a little bit about ONE of the command line utilities \nknown as the kube command line tool or kubectl or kube control as it is also called. \nThe kube control tool is used to deploy and manage applications on a kubernetes \ncluster, to get cluster information, get the status of nodes in the cluster and many \nother things. \nThe kubectl run command is used to deploy an application on the cluster. The kubectl\ncluster-info command is used to view information about the cluster and the kubectl\nget pod command is used to list all the nodes part of the cluster. That’s all we need to \nknow for now and we will keep learning more commands throughout this course. We \nwill explore more commands with kubectl when we learn the associated concepts. \nFor now just remember the run, cluster-info and get nodes commands and that will \nhelp us get through the first few labs.\n30MUMSHAD MANNAMBETH\nSetup\nmumshad mannambeth\nIn this lecture we will look at the various options available in building a Kubernetes \ncluster from scratch.\n31MUMSHAD MANNAMBETH\nSetup Kubernetes\nMinikube\nKubeadm\nGoogle Cloud Platform\nAmazon Web Services\nplay-with-k8s.com\nThere are lots of ways to setup Kuberentes. We can setup it up ourselves locally on \nour laptops or virtual machines using solutions like Minikube and Kubeadmin. \nMinikube is a tool used to setup a single instance of Kubernetes in an All-in-one setup \nand kubeadmin is a tool used to configure kubernetes in a multi-node setup. We will \nlook more into that in a bit. \nThere are also hosted solutions available for setting up kubernetes in a cloud \nenvironment such as GCP and AWS. We will also have some demos around those. \nAnd finally if you don’t have the resources or if you don’t want to go through the \nhassle of setting it all up yourself, and you simply want to get your hands on a \nkubernetes cluster instantly to play with, checkout play-with-k8s.com . I also have a \ndemo on this.\nSo feel free to chose the one that is right for you. You need not go through all the \ndemos, pick the ones that best suite your needs based on your time and resources.\n32MUMSHAD MANNAMBETH\nMinikube\nMinikube\nWorker Node\nMaster\n</> kube-apiserver </> kubelet\netcd\nContainer Runtime\nnode-controller\nreplica-controller\nWe will start with Minikube which is the easiest way to get started with Kubernetes \non a local system. If Minikube is not of interest to you, now would be a good time to \nskip this lecture. Before we head into the demo it’s good to understand how it works.  \nEarlier we talked about the different components of Kubernetes that make up a \nMaster and worker nodes such as the api server, etcd key value store, controllers and \nscheduler on the master and kubelets and container runtime on the worker nodes. It \nwould take a lot of time and effort to setup and install all of these various \ncomponents on different systems individually by ourlselves.\nMinikube bundles all of these different components into a single image providing us a \npre-configured single node kubernetes cluster so we can get started in a matter of \nminutes.  \nThe whole bundle is packaged into an ISO image and is available online for \ndownload. \n33MUMSHAD MANNAMBETH\nMinikube.exe\nMinikube\nkubectl\nSingle Node Kubernetes Cluster\nNow you don’t HAVE to download it yourself. Minikube provides an executable \ncommand line utility that will AUTOMATICALLY download the ISO and deploy it in a \nvirtualization platform such as Oracle Virtualbox or Vmware fusion. So you must have \na Hypervisor installed on your system. For windows you could use Virtualbox or \nHyper-V and for Linux use Virtualbox or KVM.\nAnd finally to interact with the kubernetes cluster, you must have the kubectl\nkubernetes command line tool also installed on your machine. So you need 3 things \nto get this working, you must have a hypervisor installed, kubectl installed and \nminikube executable installed on your system.\nSo let’s get into the demo\n34MUMSHAD MANNAMBETH\nDemo\nminikube\nThat’s it for this lecture, lets head over to the demo and see this in action.\n35MUMSHAD MANNAMBETH\nSetup - kubeadm\nmumshad mannambeth\nHello and welcome to this lecture on setting up Kubernetes with kubeadm. In this \nlecture we will look at the kubeadm tool which can be used to bootstrap a \nkubernetes cluster.\n36MUMSHAD MANNAMBETH\nkubeadm\nWorker Node 1\nMaster\n</> kube-apiserver </> kubelet\netcd\nContainer Runtime\nnode-controller\nreplica-controller\nWorker Node 2\n</> kubelet\nContainer Runtime\nWith the minikube utility you could only setup a single node kubernetes cluster. The \nkubeadmin tool helps us setup a multi node cluster with master and workers on \nseparate machines. Installing all of these various components individually on different \nnodes and modifying the configuration files to make it work is a tedious task. \nKubeadmin tool helps us in doing all of that very easily.\n37MUMSHAD MANNAMBETH\nSteps\nWorker Node 1\nMaster\n Worker Node 21\nkubeadm kubeadm kubeadm\n2\n3\nInitialize4\nPOD Network5\nJoin Node Join Node\ndocker docker docker\n6\nLet’s go through the steps – First, you must have multiple systems or virtual \nmachines created for configuring a cluster. We will see how to setup up your laptop \nto do just that if you are not familiar with it.  Once the systems are created, designate \none as master and others as worker nodes.\nThe next step is to install a container runtime on the hosts. We will be using Docker, \nso we must install Docker on all the nodes.\nThe next step is to install kubeadmin tool on all the nodes. The kubeadmin tool helps \nus bootstrap the kubernetes solution by installing and configuring all the required \ncomponents in the right nodes.\nThe third step is to initialize the Master server. During this process all the required \ncomponents are installed and configured on the master server. That way we can start \nthe cluster level configurations from the master server.\nOnce the master is initialized and before joining the worker nodes to the master, we \nmust ensure that the network pre-requisites are met. A normal network connectivity \nbetween the systems is not SUFFICIENT for this. Kubernetes requires a special \n38network between the master and worker nodes which is called as a POD network. We \nwill learn more about this network in the networking section later in this course. For \nnow we will simply follow the instructions available to get this installed and setup in \nour environment. \nThe last step is to join the worker nodes to the master node. We are then all set to \nlaunch our application in the kubernetes environment.\n38MUMSHAD MANNAMBETH\nDemo\nkubeadm\nWe will now see a demo of setting up kubernetes using the kubeadmin tool in our \nlocal environment.\n39MUMSHAD MANNAMBETH\nDemo\nGoogle Cloud Platform\nWe will now see a demo of setting up kubernetes using the kubeadmin tool in our \nlocal environment.\n40MUMSHAD MANNAMBETH\nDemo\nplay-with-k8s.com\nWe will now see a demo of setting up kubernetes using the kubeadmin tool in our \nlocal environment.\n41MUMSHAD MANNAMBETH\nPOD\nmumshad mannambeth\nHello and welcome to this lecture on Kubernetes PODs. In this lecture we will discuss \nabout Kubernetes PODs.\n42MUMSHAD MANNAMBETH\nAssumptions\nDocker Image Kubernetes Cluster\nBefore we head into understanding PODs, we would like to assume that the following \nhave been setup already. At this point, we assume that the application is already \ndeveloped and built into Docker Images and it is availalble on a Docker repository like \nDocker hub, so kubernetes can pull it down. We also assume that the Kubernetes \ncluster has already been setup and is working. This could be a single-node setup or a \nmulti-node setup, doesn’t matter. All the services need to be in a running state.\n43MUMSHAD MANNAMBETH\nNode\nPOD\nNode\nPOD\nNode\nPOD\nPOD\nAs we discussed before, with kubernetes our ultimate aim is to deploy our \napplication in the form of containers on a set of machines that are configured as \nworker nodes in a cluster. However, kubernetes does not deploy containers directly \non the worker nodes.  The containers are encapsulated into a Kubernetes object \nknown as PODs. A POD is a single instance of an application. A POD is the smallest \nobject, that you can create in kubernetes.  \n44MUMSHAD MANNAMBETH\nNode\nPOD\nNode\nPODPOD\nPOD\nKubernetes Cluster\nHere we see the simplest of simplest cases were you have a single node kubernetes \ncluster with a single instance of your application running in a single docker container \nencapsulated in a POD. What if the number of users accessing your application \nincrease and you need to scale your application? You need to add additional \ninstances of your web application to share the load. Now, were would you spin up \nadditional instances? Do we bring up a new container instance within the same \nPOD? No! We create a new POD altogether with a new instance of the same \napplication. As you can see we now have two instances of our web application \nrunning on two separate PODs on the same kubernetes system or node. \nWhat if the user base FURTHER increases and your current node has no sufficient \ncapacity? Well THEN you can always deploy additional PODs on a new node in the \ncluster. You will have a new node added to the cluster to expand the cluster’s physical \ncapacity. SO, what I am trying to illustrate in this slide is that, PODs usually have a \none-to-one relationship with containers running your application. To scale UP you \ncreate new PODs and to scale down you delete PODs. You do not add additional \ncontainers to an existing POD to scale your application. Also, if you are wondering \nhow we implement all of this and how we achieve load balancing between containers \netc, we will get into all of that in a later lecture. For now we are ONLY trying to \n45understand the basic concepts.\n45MUMSHAD MANNAMBETH\nMulti-Container PODs\nNode\nPOD\nHelper \nContainers\nNetwork\nNow we just said that PODs usually have a one-to-one relationship with the \ncontainers, but, are we restricted to having a single container in a single POD? No! A \nsingle POD CAN have multiple containers, except for the fact that they are usually not \nmultiple containers of the same kind.  As we discussed in the previous slide, if our \nintention was to scale our application, then we would need to create additional \nPODs. But sometimes you might have a scenario were you have a helper container, \nthat might be doing some kind of supporting task for our web application such as \nprocessing a user entered data, processing a file uploaded by the user etc. and you \nwant these helper containers to live along side your application container. In that \ncase, you CAN have both of these containers part of the same POD, so that when a \nnew application container is created, the helper is also created and when it dies the \nhelper also dies since they are part of the same POD. The two containers can also \ncommunicate with each other directly by referring to each other as ‘localhost’ since \nthey share the same network namespace. Plus they can easily share the same storage \nspace as well. \n46MUMSHAD MANNAMBETH\nPODs Again!\nNode\nPOD\n POD\n POD\n POD\ndocker run python-app\nNote: I am avoiding networking and load balancing details to keep explanation simple.\ndocker run python-app\ndocker run python-app\ndocker run python-app\ndocker run helper –link app1\ndocker run helper –link app2\ndocker run helper –link app3\ndocker run helper –link app4\nApp Helper\nPython1 App1\nPython2 App2\nVolume\nVol1\nVol2\nIf you still have doubts in this topic (I would understand if you did because I did the \nfirst time I learned these concepts), we could take another shot at understanding \nPODs from a different angle. Let’s, for a moment, keep kubernetes out of our \ndiscussion and talk about simple docker containers. Let’s assume we were developing \na process or a script to deploy our application on a docker host. Then we would first \nsimply deploy our application using a simple docker run python-app command and \nthe application runs fine and our users are able to access it. When the load increases \nwe deploy more instances of our application by running the docker run commands \nmany more times. This works fine and we are all happy. Now, sometime in the future \nour application is further developed, undergoes architectural changes and grows and \ngets complex. We now have new helper containers that helps our web applications by \nprocessing or fetching data from elsewhere.  These helper containers maintain a one-\nto-one relationship with our application container and thus, needs to communicate \nwith the application containers directly and access data from those containers. For \nthis we need to maintain a map of what app and helper containers are connected to \neach other, we would need to establish network connectivity between these \ncontainers ourselves using links and custom networks, we would need to create \nshareable volumes and share it among the containers and maintain a map of that as \nwell. And most importantly we would need to monitor the state of the application \n47container and when it dies, manually kill the helper container as well as its no longer \nrequired. When a new container is deployed we would need to deploy the new \nhelper container as well. \nWith PODs, kubernetes does all of this for us automatically. We just need to define \nwhat containers a POD consists of and the containers in a POD by default will have \naccess to the same storage, the same network namespace, and same fate as in they \nwill be created together and destroyed together. \nEven if our application didn’t happen to be so complex and we could live with a single \ncontainer, kubernetes still requires you to create PODs. But this is good in the long \nrun as your application is now equipped for architectural changes and scale in the \nfuture. \nHowever, multi-pod containers are a rare use-case and we are going to stick to single \ncontainer per POD in this course.\n47MUMSHAD MANNAMBETH\nkubectl\nkubectl run nginx\nNode\nPOD\nkubectl get pods\n–-image nginx\nLet us now look at how to deploy PODs. Earlier we learned about the kubectl run \ncommand. What this command really does is it deploys a docker container by \ncreating a POD. So it first creates a POD automatically and deploys an instance of the \nnginx docker image. But were does it get the application image from? For that you \nneed to specify the image name using the –-image parameter. The application image, \nin this case the nginx image, is downloaded from the docker hub repository. Docker \nhub as we discussed is a public repository were latest docker images of various \napplications are stored. You could configure kubernetes to pull the image from the \npublic docker hub or a private repository within the organization. \nNow that we have a POD created, how do we see the list of PODs available? The \nkubectl get PODs command helps us see the list of pods in our cluster. In this case we \nsee the pod is in a ContainerCreating state and soon changes to a Running state when \nit is actually running.\nAlso remember that we haven’t really talked about the concepts on how a user can \naccess the nginx web server. And so in the current state we haven’t made the web \nserver accessible to external users. You can access it internally from the Node though. \nFor now we will just see how to deploy a POD and in a later lecture once we learn \n48about networking and services we will get to know how to make this service \naccessible to end users.\n48MUMSHAD MANNAMBETH\nDemo\nPOD\nThat’s it for this lecture. We will now head over to a Demo and I will see you in the \nnext lecture.\n49MUMSHAD MANNAMBETH\nPOD\nWith YAML\nHello and welcome to this lecture, In this lecture we will talk about creating a POD \nusing a YAML based configuration file.\n50MUMSHAD MANNAMBETH\nYAML in Kubernetes\napiVersion: \nkind:\nmetadata:\nspec:\npod-definition.yml\nv1\nPod\nname: myapp-pod\nlabels:\napp: myapp\ntype: front-end\ncontainers:\n- name: nginx-container\nimage: nginx\nkubectl create –f pod-definition.yml\nString\nString\nDictionary\nKind Version\nPOD v1\nService v1\nReplicaSet apps/v1\nDeployment apps/v1\nmetadata:\nname: myapp-pod\nlabels:\napp: myapp\nList/Array\n1st Item in List\nIn the previous lecture we learned about YAML files in general. Now we will learn \nhow to develop YAML files specifically for Kubernetes. Kubernetes uses YAML files as \ninput for the creation of objects such as PODs, Replicas, Deployments, Services etc. \nAll of these follow similar structure. A kubernetes definition file always contains 4 top \nlevel fields. The apiVersion, kind, metadata and spec. These are top level or root level \nproperties. Think of them as siblings, children of the same parent. These are all \nREQUIRED fields, so you MUST have them in your configuration file. \nLet us look at each one of them. The first one is the apiVersion. This is the version of \nthe kubernetes API we’re using to create the object.  Depending on what we are \ntrying to create we must use the RIGHT apiVersion. For now since we are working on \nPODs, we will set the apiVersion as v1. Few other possible values for this field are \napps/v1beta1, extensions/v1beta1 etc. We will see what these are for later in this \ncourse.\nNext is the kind. The kind refers to the type of object we are trying to create, which in \nthis case happens to be a POD. So we will set it as Pod. Some other possible values \nhere could be ReplicaSet or Deployment or Service, which is what you see in the kind \nfield in the table on the right.\n51The next is metadata. The metadata is data about the object like its name, labels etc.  \nAs you can see unlike the first two were you specified a string value, this, is in the \nform of a dictionary. So everything under metadata is intended to the right a little bit \nand so names and labels are children of metadata.  The number of spaces before the \ntwo properties name and labels doesn’t matter, but they should be the same as they \nare siblings. In this case labels has more spaces on the left than name and so it is now \na child of the name property instead of a sibling. Also the two properties must have \nMORE spaces than its parent, which is metadata, so that its intended to the right a \nlittle bit. In this case all 3 have the same number of spaces before them and so they \nare all siblings, which is not correct. Under metadata, the name is a string value – so \nyou can name your POD myapp-pod - and the labels is a dictionary.  So labels is a \ndictionary within the metadata dictionary. And it can have any key and value pairs as \nyou wish. For now I have added a label app with the value myapp. Similarly you could \nadd other labels as you see fit which will help you identify these objects at a later \npoint in time. Say for example there are 100s of PODs running a front-end \napplication, and 100’s of them running a backend application or a database, it will be \nDIFFICULT for you to group these PODs once they are deployed. If you label them \nnow as front-end, back-end or database, you will be able to filter the PODs based on \nthis label at a later point in time. \nIt’s IMPORTANT to note that under metadata, you can only specify name or labels or \nanything else that kubernetes expects to be under metadata. You CANNOT add any \nother property as you wish under this. However, under labels you CAN have any kind \nof key or value pairs as you see fit. So its IMPORTANT to understand what each of \nthese parameters expect.  \nSo far we have only mentioned the type and name of the object we need to create \nwhich happens to be a POD with the name myapp-pod, but we haven’t really \nspecified the container or image we need in the pod. The last section in the \nconfiguration file is the specification which is written as spec. Depending on the \nobject we are going to create, this is were we provide additional information to \nkubernetes pertaining to that object. This is going to be different for different objects, \nso its important to understand or refer to the documentation section to get the right \nformat for each. Since we are only creating a pod with a single container in it, it is \neasy. Spec is a dictionary so add a property under it called containers, which is a list \nor an array. The reason this property is a list is because the PODs can have multiple \ncontainers within them as we learned in the lecture earlier. In this case though, we \nwill only add a single item in the list, since we plan to have only a single container in \nthe POD. The item in the list is a dictionary, so add a name and image property. The \nvalue for image is nginx.  \n51Once the file is created, run the command kubectl create -f followed by the file name \nwhich is pod-definition.yml and kubernetes creates the pod.\nSo to summarize remember the 4 top level properties. apiVersion, kind, metadata \nand spec. Then start by adding values to those depending on the object you are \ncreating. \n51MUMSHAD MANNAMBETH\nCommands\n> kubectl get pods\nNAME             READY     STATUS    RESTARTS   AGE\nmyapp-pod        1/1       Running   0          20s\n> kubectl describe pod myapp-pod\nName:         myapp-pod\nNamespace:    default\nNode:         minikube/192.168.99.100\nStart Time:   Sat, 03 Mar 2018 14:26:14 +0800\nLabels:       app=myapp\nname=myapp-pod\nAnnotations:  <none>\nStatus:       Running\nIP:           10.244.0.24\nContainers:\nnginx:\nContainer ID:   docker://830bb56c8c42a86b4bb70e9c1488fae1bc38663e4918b6c2f5a783e7688b8c9d\nImage:          nginx\nImage ID:       docker-pullable://nginx@sha256:4771d09578c7c6a65299e110b3ee1c0a2592f5ea2618d23e4ffe7a4cab1ce5de\nPort:           <none>\nState:          Running\nStarted:      Sat, 03 Mar 2018 14:26:21 +0800\nReady:          True\nRestart Count:  0\nEnvironment:    <none>\nMounts:\n/var/run/secrets/kubernetes.io/serviceaccount from default-token-x95w7 (ro)\nConditions:\nType           Status\nInitialized    True\nReady          True\nPodScheduled True\nEvents:\nType    Reason                 Age   From               Message\n---- ------ ---- ---- -------\nNormal  Scheduled              34s   default-scheduler  Successfully assigned myapp-pod to minikube\nNormal  SuccessfulMountVolume 33s   kubelet, minikube MountVolume.SetUp succeeded for volume "default-token-x95w7"\nNormal  Pulling                33s   kubelet, minikube pulling image "nginx"\nNormal  Pulled                 27s   kubelet, minikube Successfully pulled image "nginx"\nNormal  Created                27s   kubelet, minikube Created container\nNormal  Started                27s   kubelet, minikube Started container\nOnce we create the pod, how do you see it? Use the kubectl get pods command to \nsee a list of pods available. In this case its just one. To see detailed information about \nthe pod run the kubectl describe pod command.  This will tell you information about \nthe POD, when it was created, what labels are assigned to it, what docker containers \nare part of it and the events associated with that POD. \n52MUMSHAD MANNAMBETH\nDemo\nPOD Using YAML\nThat’s it for this lecture. We will now head over to a Demo and I will see you in the \nnext lecture.\n53MUMSHAD MANNAMBETH\nTips & \nTricks\nWorking YAML Files\nThat’s it for this lecture. We will now head over to a Demo and I will see you in the \nnext lecture.\n54MUMSHAD MANNAMBETH\nResources\nLink to Versions and Groups - https://kubernetes.io/docs/reference/generated/kubernetes-\napi/v1.9/#replicaset-v1-apps\nhttps://plugins.jetbrains.com/plugin/9354-kubernetes-and-openshift-resource-support\n55MUMSHAD MANNAMBETH\nReplication \nController\nmumshad mannambeth\nHello and welcome to this lecture on Kubernetes Controllers. In this lecture we will \ndiscuss about Kubernetes Controllers. Controllers are the brain behind Kubernetes. \nThey are processes that monitor kubernetes objects and respond accordingly.  In this \nlecture we will discuss about one controller in particular. And that is the Replication \nController.\n56MUMSHAD MANNAMBETH\nNode\nReplication Controller\nNode\nReplication Controller\nPOD POD\nHigh Availability\nPOD\nPOD\nSo what is a replica and why do we need a replication controller? Let’s go back to our \nfirst scenario were we had a single POD running our application. What if for some \nreason, our application crashes and the POD fails? Users will no longer be able to \naccess our application. To prevent users from losing access to our application, we \nwould like to have more than one instance or POD running at the same time. That \nway if one fails we still have our application running on the other one. The replication \ncontroller helps us run multiple instances of a single POD in the kubernetes cluster \nthus providing High Availability. \nSo does that mean you can’t use a replication controller if you plan to have a single \nPOD? No! Even if you have a single POD, the replication controller can help by \nautomatically bringing up a new POD when the existing one fails. Thus the replication \ncontroller ensures that the specified number of PODs are running at all times. Even if \nit’s just 1 or 100.\n57MUMSHAD MANNAMBETH\nNode\n Node\nReplication Controller\nLoad Balancing & Scaling\nReplication Controller\nPOD POD\n POD POD\nAnother reason we need replication controller is to create multiple PODs to share the \nload across them.  For example, in this simple scenario we have a single POD serving \na set of users. When the number of users increase we deploy additional POD to \nbalance the load across the two pods. If the demand further increases and If we \nwere to run out of resources on the first node, we could deploy additional PODs \nacross other nodes in the cluster. As you can see, the replication controller spans \nacross multiple nodes in the cluster. It helps us balance the load across multiple pods \non different nodes as well as scale our application when the demand increases.\n58MUMSHAD MANNAMBETH\nReplication Controller Replica Set\nIt’s important to note that there are two similar terms. Replication Controller and \nReplica Set. Both have the same purpose but they are not the same. Replication \nController is the older technology that is being replaced by Replica Set. Replica set is \nthe new recommended way to setup replication. However, whatever we discussed in \nthe previous few slides remain applicable to both these technologies. There are \nminor differences in the way each works and we will look at that in a bit.  \nAs such we will try to stick to Replica Sets in all of our demos and implementations \ngoing forward.\n59MUMSHAD MANNAMBETH\napiVersion: \nkind:\nmetadata:\nspec:\nrc-definition.yml\nv1\nReplicationController\nname: myapp-rc\nlabels:\napp: myapp\ntype: front-end\n> kubectl create –f rc-definition.yml\napiVersion: \nkind:\npod-definition.yml\nv1\nPod\ntemplate:\nPOD\nmetadata:\nname: myapp-pod\nlabels:\napp: myapp\ntype: front-end\nspec:\ncontainers:\n- name: nginx-container\nimage: nginx\nPOD\nreplicas:\n 3\n> kubectl get replicationcontroller\nreplicationcontroller “myapp-rc” created\nNAME       DESIRED   CURRENT   READY     AGE\nmyapp-rc 3         3         3         19s\n> kubectl get pods\nNAME             READY     STATUS    RESTARTS   AGE\nmyapp-rc-4lvk9   1/1       Running   0          20s\nmyapp-rc-mc2mf   1/1       Running   0          20s\nmyapp-rc-px9pz   1/1       Running   0          20s\nReplication Controller\nPOD\nReplication Controller\nPOD\nLet us now look at how we create a replication controller. As with the previous \nlecture, we start by creating a replication controller definition file. We will name it rc-\ndefinition.yml. As with any kubernetes definition file, we will have 4 sections. The \napiVersion, kind, metadata and spec. The apiVersion is specific to what we are \ncreating. In this case replication controller is supported in kubernetes apiVersion v1. \nSo we will write it as v1. The kind as we know is ReplicationController. Under \nmetadata, we will add a name and we will call it myapp-rc. And we will also add a few \nlabels, app and type and assign values to them. So far, it has been very similar to how \nwe created a POD in the previous section. The next is the most crucial part of the \ndefinition file and that is the specification written as spec. For any kubernetes \ndefinition file, the spec section defines what’s inside the object we are creating. In \nthis case we know that the replication controller creates multiple instances of a POD. \nBut what POD? We create a template section under spec to provide a POD template \nto be used by the replication controller to create replicas. Now how do we DEFINE \nthe POD template? It’s not that hard because, we have already done that in the \nprevious exercise. Remember, we created a pod-definition file in the previous \nexercise. We could re-use the contents of the same file to populate the template \nsection. Move all the contents of the pod-definition file into the template section of \nthe replication controller, except for the first two lines – which are apiVersion and \n60kind. Remember whatever we move must be UNDER the template section. Meaning, \nthey should be intended to the right and have more spaces before them than the \ntemplate line itself. Looking at our file, we now have two metadata sections – one is \nfor the Replication Controller and another for the POD and we have two spec \nsections – one for each. We have nested two definition files together. The replication \ncontroller being the parent and the pod-definition being the child. \nNow, there is something still missing. We haven’t mentioned how many replicas we \nneed in the replication controller. For that, add another property to the spec called \nreplicas and input the number of replicas you need under it. Remember that the \ntemplate and replicas are direct children of the spec section. So they are siblings and \nmust be on the same vertical line : having equal number of spaces before them.\nOnce the file is ready, run the kubectl create command and input the file using the –f \nparameter. The replication controller Is created. When the replication controller is \ncreated it first creates the PODs using the pod-definition template as many as \nrequired, which is 3 in this case. To view the list of created replication controllers run \nthe kubectl get replication controller command and you will see the replication \ncontroller listed. We can also see the desired number of replicas or pods, the current \nnumber of replicas and how many of them are ready. If you would like to see the \npods that were created by the replication controller, run the kubectl get pods \ncommand and you will see 3 pods running. Note that all of them are starting with the \nname of the replication controller which is myapp-rc indicating that they are all \ncreated automatically by the replication controller.\n60MUMSHAD MANNAMBETH\napiVersion: \nkind:\nmetadata:\nspec:\nreplicaset-definition.yml\napps/v1\nReplicaSet\nname: myapp-replicaset\nlabels:\napp: myapp\ntype: front-end\n> kubectl create –f replicaset-definition.yml\napiVersion: \nkind:\npod-definition.yml\nv1\nPod\ntemplate:\nPOD\nmetadata:\nname: myapp-pod\nlabels:\napp: myapp\ntype: front-end\nspec:\ncontainers:\n- name: nginx-container\nimage: nginx\nPOD\nreplicas:\n 3\nselector:\nmatchLabels:\ntype: front-end\nreplicaset "myapp-replicaset" created\n> kubectl get replicaset\nNAME       DESIRED   CURRENT    READY       AGE\nmyapp-replicaset 3         3         3         19s\n> kubectl get pods\nNAME                    READY      STATUS    RESTARTS   AGE\nmyapp-replicaset-9ddl9   1/1       Running   0          45s\nmyapp-replicaset-9jtpx   1/1       Running   0          45s\nmyapp-replicaset-hq84m   1/1       Running   0          45s\nerror: unable to recognize "replicaset-\ndefinition.yml": no matches for /, Kind=ReplicaSet\nWhat we just saw was ReplicationController. Let us now look at ReplicaSet. It is very \nsimilar to replication controller. As usual, first we have apiVersion, kind, metadata \nand spec. The apiVersion though is a bit different. It is apps/v1 which is different from \nwhat we had before for replication controller. For replication controller it was simply \nv1. If you get this wrong, you are likely to get an error that looks like this. It would say \nno match for kind ReplicaSet, because the specified kubernetes api version has no \nsupport for ReplicaSet. \nThe kind would be ReplicaSet and we add name and labels in metadata. \nThe specification section looks very similar to replication controller. It has a template \nsection were we provide pod-definition as before. So I am going to copy contents \nover from pod-definition file. And we have number of replicas set to 3. However, \nthere is one major difference between replication controller and replica set. Replica \nset requires a selector definition.  The selector section helps the replicaset identify \nwhat pods fall under it. But why would you have to specify what PODs fall under it, if \nyou have provided the contents of the pod-definition file itself in the template? It’s \nBECAUSE, replica set can ALSO manage pods that were not created as part of the \n61replicaset creation. Say for example, there were pods created BEFORE the creation of \nthe ReplicaSet that match the labels specified in the selector, the replica set will also \ntake THOSE pods into consideration when creating the replicas. I will elaborate this in \nthe next slide.\nBut before we get into that, I would like to mention that the selector is one of the \nmajor differences between replication controller and replica set. The selector is not a \nREQUIRED field in case of a replication controller, but it is still available. When you \nskip it, as we did in the previous slide, it assumes it to be the same as the labels \nprovided in the pod-definition file. In case of replica set a user input IS required for \nthis property. And it has to be written in the form of matchLabels as shown here. The \nmatchLabels selector simply matches the labels specified under it to the labels on the \nPODs. The replicaset selector also provides many other options for matching labels \nthat were not available in a replication controller.\nAnd as always to create a ReplicaSet run the kubectl create command providing the \ndefinition file as input and to see the created replicasets run the kubectl get \nreplicaset command. To get list of pods, simply run the kubectl get pods command.\n61MUMSHAD MANNAMBETH\nPOD\nPOD\nPOD\ntier: front-end\ntier: front-end\ntier: front-end\nLabels and Selectors\nPOD\nPOD\nPOD\nPOD\nPOD\nPOD\nPOD\nPOD\nPOD\nPOD\nPOD\nPOD\nPOD\nPOD\nPOD\nPOD\nPOD\nPOD\nPOD\nPOD\nPOD\nmetadata:\nname: myapp-pod\nlabels:\ntier: front-end\nselector:\nmatchLabels:\ntier: front-end\nreplicaset-definition.yml pod-definition.yml\nSo what is the deal with Labels and Selectors? Why do we label our PODs and objects \nin kubernetes? Let us look at a simple scenario. Say we deployed 3 instances of our \nfrontend web application as 3 PODs. We would like to create a replication controller \nor replica set to ensure that we have 3 active PODs at anytime. And YES that is one of \nthe use cases of replica sets. You CAN use it to monitor existing pods, if you have \nthem already created, as it IS in this example. In case they were not created, the \nreplica set will create them for you. The role of the replicaset is to monitor the pods \nand if any of them were to fail, deploy new ones. The replica set is in FACT a process \nthat monitors the pods. Now, how does the replicaset KNOW what pods to monitor. \nThere could be 100s of other PODs in the cluster running different application. This is \nwere labelling our PODs during creation comes in handy. We could now provide \nthese labels as a filter for replicaset. Under the selector section we use the \nmatchLabels filter and provide the same label that we used while creating the pods. \nThis way the replicaset knows which pods to monitor.\n62MUMSHAD MANNAMBETH\napiVersion: \nkind:\nmetadata:\nspec:\nreplicaset-definition.yml\nReplicaSet\nname: myapp-replicaset\nlabels:\napp: myapp\ntype: front-end\napps/v1\nmetadata:\nname: myapp-pod\nlabels:\napp: myapp\ntype: front-end\nspec:\ncontainers:\n- name: nginx-container\nimage: nginx\nreplicas:\nselector:\nmatchLabels:\ntype: front-end\ntemplate:\n3\nPOD\nPOD\nPOD\ntier: front-end\ntier: front-end\ntier: front-end\nTemplate\nNow let me ask you a question along the same lines. In the replicaset specification \nsection we learned that there are 3 sections: Template, replicas and the selector. We \nneed 3 replicas and we have updated our selector based on our discussion in the \nprevious slide. Say for instance we have the same scenario as in the previous slide \nwere we have 3 existing PODs that were created already and we need to create a \nreplica set to monitor the PODs to ensure there are a minimum of 3 running at all \ntimes. When the replication controller is created, it is NOT going to deploy a new \ninstance of POD as 3 of them with matching labels are already created.  In that case, \ndo we really need to provide a template section in the replica-set specification, since \nwe are not expecting the replicaset to create a new POD on deployment? Yes we do, \nBECAUSE in case one of the PODs were to fail in the future, the replicaset needs to \ncreate a new one to maintain the desired number of PODs. And for the replica set to \ncreate a new POD, the template definition section IS required.\n63MUMSHAD MANNAMBETH\nScale apiVersion: \nkind:\nmetadata:\nspec:\nreplicaset-definition.yml\nReplicaSet\nname: myapp-replicaset\nlabels:\napp: myapp\ntype: front-end\napps/v1\nmetadata:\nname: myapp-pod\nlabels:\napp: myapp\ntype: front-end\nspec:\ncontainers:\n- name: nginx-container\nimage: nginx\nreplicas:\nselector:\nmatchLabels:\ntype: front-end\ntemplate:\n3\n> kubectl replace -f replicaset-definition.yml\n6\n> kubectl scale -–replicas=6 –f replicaset-definition.yml\n> kubectl scale -–replicas=6 replicaset myapp-replicaset\nTYPE              NAME\nLet’s look at how we scale the replicaset. Say we started with 3 replicas and in the \nfuture we decide to scale to 6. How do we update our replicaset to scale to 6 replicas. \nWell there are multiple ways to do it. The first, is to update the number of replicas in \nthe definition file to 6. Then run the kubectl replace command specifying the same \nfile using the –f parameter and that will update the replicaset to have 6 replicas.  \nThe second way to do it is to run the kubectl scale command.  Use the replicas \nparameter to provide the new number of replicas and specify the same file as input. \nYou may either input the definition file or provide the replicaset name in the TYPE \nName format. However, Remember that using the file name as input will not result in \nthe number of replicas being updated automatically in the file.  In otherwords, the \nnumber of replicas in the replicaset-definition file will still be 3  even though you \nscaled your replicaset to have 6 replicas using the kubectl scale command and the file \nas input.\nThere are also options available for automatically scaling the replicaset based on \nload, but that is an advanced topic and we will discuss it at a later time.\n64MUMSHAD MANNAMBETH\ncommands\n> kubectl create –f replicaset-definition.yml\n> kubectl get replicaset\n> kubectl delete replicaset myapp-replicaset *Also deletes all underlying PODs\n> kubectl replace -f replicaset-definition.yml\n> kubectl scale –replicas=6 -f replicaset-definition.yml\nLet us now review the commands real quick. The kubectl create command, as we \nknow, is used to create a replca set. You must provide the input file using the –f \nparameter. Use the kubectl get command to see list of replicasets created. Use the \nkubectl delete replicaset command followed by the name of the replica set to delete \nthe replicaset. And then we have the kubectl replace command to replace or update \nreplicaset and also the kubectl scale command to scale the replicas simply from the \ncommand line without having to modify the file.\n65MUMSHAD MANNAMBETH\nDemo\nReplicaSet\nThat’s it for this lecture. We will now head over to a Demo and I will see you in the \nnext lecture.\n66MUMSHAD MANNAMBETH\nReferences\nReplicaSet as a Horizontal Pod Autoscaler Target\nhttps://kubernetes.io/docs/concepts/workloads/controllers/replicaset/#replicaset-as-an-\nhorizontal-pod-autoscaler-target\n67MUMSHAD MANNAMBETH\nDeployment\nmumshad mannambeth\nIn this lecture we will discuss about Kubernetes Deployments.\n68MUMSHAD MANNAMBETH\nDeployment\nReplica Set\nPOD POD POD POD POD POD PODPODPOD\nv1 v2\nDeployment\nFor a minute, let us forget about PODs and replicasets and other kubernetes concepts \nand talk about how you might want to deploy your application in a production \nenvironment. Say for example you have a web server that needs to be deployed in a \nproduction environment. You need not ONE, but many such instances of the web \nserver running for obvious reasons. \nSecondly, when newer versions of application builds become available on the docker \nregistry, you would like to UPGRADE your docker instances seamlessly. \nHowever, when you upgrade your instances, you do not want to upgrade all of them \nat once as we just did. This may impact users accessing our applications, so you may \nwant to upgrade them one after the other. And that kind of upgrade is known as \nRolling Updates.\nSuppose one of the upgrades you performed resulted in an unexpected error and you \nare asked to undo the recent update. You would like to be able to rollBACK the \nchanges that were recently carried out.\nFinally, say for example you would like to make multiple changes to your environment \n69such as upgrading the underlying WebServer versions, as well as scaling your \nenvironment and also modifying the resource allocations etc. You do not want to \napply each change immediately after the command is run, instead you would like to \napply a pause to your environment, make the changes and then resume so that all \nchanges are rolled-out together.\nAll of these capabilities are available with the kubernetes Deployments. \nSo far in this course we discussed about PODs, which deploy single instances of our \napplication such as the web application in this case. Each container is encapsulated in \nPODs. Multiple such PODs are deployed using Replication Controllers or Replica Sets.  \nAnd then comes Deployment which is a kubernetes object that comes higher in the \nhierarchy. The deployment provides us with capabilities to upgrade the underlying \ninstances seamlessly using rolling updates, undo changes, and pause and resume \nchanges to deployments.\n69MUMSHAD MANNAMBETH\nDefinition\napiVersion: \nkind:\nmetadata:\nspec:\ndeployment-definition.yml\nReplicaSet\nname: myapp-deployment\nlabels:\napp: myapp\ntype: front-end\napps/v1\nmetadata:\nname: myapp-pod\nlabels:\napp: myapp\ntype: front-end\nspec:\ncontainers:\n- name: nginx-container\nimage: nginx\nreplicas:\nselector:\nmatchLabels:\ntype: front-end\ntemplate:\n3\n> kubectl create –f deployment-definition.yml\ndeployment "myapp-deployment" created\n> kubectl get deployments\nNAME               DESIRED   CURRENT   UP-TO-DATE   AVAILABLE   AGE\nmyapp-deployment   3         3         3            3           21s\n> kubectl get replicaset\nNAME                          DESIRED   CURRENT   READY     AGE\nmyapp-deployment-6795844b58   3         3         3         2m\n> kubectl get pods\nNAME                                READY     STATUS    RESTARTS   AGE\nmyapp-deployment-6795844b58-5rbjl   1/1       Running   0          2m\nmyapp-deployment-6795844b58-h4w55   1/1       Running   0          2m\nmyapp-deployment-6795844b58-lfjhv   1/1       Running   0          2m\nSo how do we create a deployment. As with the previous components, we first \ncreate a deployment definition file.  The contents of the deployment-definition file \nare exactly similar to the replicaset definition file, except for the kind, which is now \ngoing to be Deployment.\nIf we walk through the contents of the file it has an apiVersion which is apps/v1, \nmetadata which has name and labels and a spec that has template, replicas and \nselector. The template has a POD definition inside it. \nOnce the file is ready run the kubectl create command and specify deployment \ndefinition file. Then run the kubectl get deployments command to see the newly \ncreated deployment.  The deployment automatically creates a replica set. So if you \nrun the kubectl get replcaset command you will be able to see a new replicaset in the \nname of the deployment. The replicasets ultimately create pods, so if you run the \nkubectl get pods command you will be able to see the pods with the name of the \ndeployment and the replicaset.\nSo far there hasn’t been much of a difference between replicaset and deployments, \nexcept for the fact that deployments created a new kubernetes object called \n70deployments. We will see how to take advantage of the deployment using the use \ncases we discussed in the previous slide in the upcoming lectures.\n70MUMSHAD MANNAMBETH\ncommands\n> kubectl get all\nNAME                      DESIRED   CURRENT   UP-TO-DATE   AVAILABLE   AGE\ndeploy/myapp-deployment   3         3         3            3           9h\nNAME                             DESIRED   CURRENT   READY     AGE\nrs/myapp-deployment-6795844b58   3         3         3         9h\nNAME                                   READY     STATUS    RESTARTS   AGE\npo/myapp-deployment-6795844b58-5rbjl   1/1       Running   0          9h\npo/myapp-deployment-6795844b58-h4w55   1/1       Running   0          9h\npo/myapp-deployment-6795844b58-lfjhv   1/1       Running   0          9h\nTo see all the created objects at once run the kubectl get all command.\n71MUMSHAD MANNAMBETH\nDemo\nDeployment\nThat’s it for this lecture. We will now head over to a Demo and I will see you in the \nnext lecture.\n72MUMSHAD MANNAMBETH\nDeployment\nUpdates and Rollback\nmumshad mannambeth\nIn this lecture we will talk about updates and rollbacks in a Deployment.\n73MUMSHAD MANNAMBETH\nRollout and Versioning\nRevision 1\nRevision 2\nnginx:1.7.0 nginx:1.7.0 nginx:1.7.0 nginx:1.7.0 nginx:1.7.0 nginx:1.7.0 nginx:1.7.0 nginx:1.7.0 nginx:1.7.0\nnginx:1.7.1 nginx:1.7.1 nginx:1.7.1 nginx:1.7.1 nginx:1.7.1 nginx:1.7.1 nginx:1.7.1 nginx:1.7.1 nginx:1.7.1\nBefore we look at how we upgrade our application, let’s try to understand Rollouts \nand Versioning in a deployment. Whenever you create a new deployment or upgrade \nthe images in an existing deployment it triggers a Rollout. A rollout is the process of \ngradually deploying or upgrading your application containers. When you first create a \ndeployment, it triggers a rollout. A new rollout creates a new Deployment revision. \nLet’s call it revision 1. In the future when the application is upgraded – meaning \nwhen the container version is updated to a new one – a new rollout is triggered and a \nnew deployment revision is created named Revision 2. This helps us keep track of the \nchanges made to our deployment and enables us to rollback to a previous version of \ndeployment if necessary.\n74MUMSHAD MANNAMBETH\nRollout Command\n> kubectl rollout status deployment/myapp-deployment\nWaiting for rollout to finish: 0 of 10 updated replicas are available...\nWaiting for rollout to finish: 1 of 10 updated replicas are available...\nWaiting for rollout to finish: 2 of 10 updated replicas are available...\nWaiting for rollout to finish: 3 of 10 updated replicas are available...\nWaiting for rollout to finish: 4 of 10 updated replicas are available...\nWaiting for rollout to finish: 5 of 10 updated replicas are available...\nWaiting for rollout to finish: 6 of 10 updated replicas are available...\nWaiting for rollout to finish: 7 of 10 updated replicas are available...\nWaiting for rollout to finish: 8 of 10 updated replicas are available...\nWaiting for rollout to finish: 9 of 10 updated replicas are available...\ndeployment "myapp-deployment" successfully rolled out\n> kubectl rollout history deployment/myapp-deployment\ndeployments "myapp-deployment"\nREVISION  CHANGE-CAUSE\n1         <none>\n2         kubectl apply --filename=deployment-definition.yml --record=true\nYou can see the status of your rollout by running the command: kubectl rollout status \nfollowed by the name of the deployment. \nTo see the revisions and history of rollout run the command kubectl rollout history \nfollowed by the deployment name and this will show you the revisions.\n75MUMSHAD MANNAMBETH\nDeployment Strategy\nnginx:1.7.0 nginx:1.7.0 nginx:1.7.0 nginx:1.7.0 nginx:1.7.0\n nginx:1.7.1 nginx:1.7.1 nginx:1.7.1 nginx:1.7.1 nginx:1.7.1\nApplication\nDown\nRecreate\nnginx:1.7.0 nginx:1.7.0 nginx:1.7.0 nginx:1.7.0 nginx:1.7.0\nnginx:1.7.1 nginx:1.7.1 nginx:1.7.1 nginx:1.7.1 nginx:1.7.1Rolling \nUpdate\nThere are two types of deployment strategies.  Say for example you have 5 replicas of \nyour web application instance deployed.  One way to upgrade these to a newer \nversion is to destroy all of these and then create newer versions of application \ninstances. Meaning first, destroy the 5 running instances and then deploy 5 new \ninstances of the new application version. The problem with this as you can imagine, \nis that during the period after the older versions are down and before any newer \nversion is up, the application is down and inaccessible to users. This strategy is \nknown as the Recreate strategy, and thankfully this is NOT the default deployment \nstrategy.\nThe second strategy is were we do not destroy all of them at once. Instead we take \ndown the older version and bring up a newer version one by one. This way the \napplication never goes down and the upgrade is seamless.\nRemember, if you do not specify a strategy while creating the deployment, it will \nassume it to be Rolling Update. In other words, RollingUpdate is the default \nDeployment Strategy.\n76MUMSHAD MANNAMBETH\nKubectl apply\napiVersion: \nkind:\nmetadata:\nspec:\ndeployment-definition.yml\nname: myapp-deployment\nlabels:\napp: myapp\ntype: front-end\napps/v1\nmetadata:\nname: myapp-pod\nlabels:\napp: myapp\ntype: front-end\nspec:\ncontainers:\n- name: nginx-container\nimage: nginx\nreplicas:\nselector:\nmatchLabels:\ntype: front-end\ntemplate:\n3\nDeployment\n:1.7.1\n> kubectl apply –f deployment-definition.yml\ndeployment "myapp-deployment" configured\n> kubectl set image deployment/myapp-deployment \\\nnginx=nginx:1.9.1\ndeployment "myapp-deployment" image is updated\nSo we talked about upgrades. How exactly DO you update your deployment? When I \nsay update it could be different things such as updating your application version by \nupdating the version of docker containers used, updating their labels or updating the \nnumber of replicas etc. Since we already have a deployment definition file it is easy \nfor us to modify this file. Once we make the necessary changes, we run the kubectl\napply command to apply the changes. A new rollout is triggered and a new revision of \nthe deployment is created.\nBut there is ANOTHER way to do the same thing. You could use the kubectl set image \ncommand to update the image of your application. But remember, doing it this way \nwill result in the deployment-definition file having a different configuration. So you \nmust be careful when using the same definition file to make changes in the future.\n77MUMSHAD MANNAMBETH\nRecreate                          RollingUpdate\nThe difference between the recreate and rollingupdate strategies can also be seen \nwhen you view the deployments in detail. Run the kubectl describe deployment \ncommand to see detailed information regarding the deployments. You will notice \nwhen the Recreate strategy was used the events indicate that the old replicaset was \nscaled down to 0 first and the new replica set scaled up to 5. However when the \nRollingUpdate strategy was used the old replica set was scaled down one at a time \nsimultaneously scaling up the new replica set one at a time.\n78MUMSHAD MANNAMBETH\nUpgrades\nDeployment\nReplica Set - 1\nPOD\n POD\n POD\n POD\n POD\nReplica Set - 2\nPOD\n POD\n POD\n POD\n POD\n> kubectl get replicasets\nNAME                          DESIRED   CURRENT   READY     AGE\nmyapp-deployment-67c749c58c   0         0         0         22m\nmyapp-deployment-7d57dbdb8d   5         5         5         20m\nLet’s look at how a deployment performs an upgrade under the hoods.  When a new \ndeployment is created, say to deploy 5 replicas, it first creates a Replicaset\nautomatically, which in turn creates the number of PODs required to meet the \nnumber of replicas. When you upgrade your application as we saw in the previous \nslide, the kubernetes deployment object creates a NEW replicaset under the hoods \nand starts deploying the containers there. At the same time taking down the PODs in \nthe old replica-set following a RollingUpdate strategy. \nThis can be seen when you try to list the replicasets using the kubectl get replicasets\ncommand. Here we see the old replicaset with 0 PODs and the new replicaset with 5 \nPODs.\n79MUMSHAD MANNAMBETH\nRollback\nDeployment\nReplica Set - 1\nPOD\n POD\n POD\n POD\n POD\nReplica Set - 2\nPOD\n POD\n POD\n POD\n POD\n> kubectl rollout undo deployment/myapp-deployment\ndeployment “myapp-deployment” rolled back\n> kubectl get replicasets\nNAME                          DESIRED   CURRENT   READY     AGE\nmyapp-deployment-67c749c58c   0         0         0         22m\nmyapp-deployment-7d57dbdb8d   5         5         5         20m\n> kubectl get replicasets\nNAME                          DESIRED   CURRENT   READY     AGE\nmyapp-deployment-67c749c58c   5         5         5         22m\nmyapp-deployment-7d57dbdb8d   0         0         0         20m\nSay for instance once you upgrade your application, you realize something isn’t very \nright. Something’s wrong with the new version of build you used to upgrade. So you \nwould like to rollback your update. Kubernetes deployments allow you to rollback to \na previous revision. To undo a change run the command kubectl rollout undo \nfollowed by the name of the deployment. The deployment will then destroy the \nPODs in the new replicaset and bring the older ones up in the old replicaset. And your \napplication is back to its older format.\nWhen you compare the output of the kubectl get replicasets command, before and \nafter the rollback, you will be able to notice this difference. Before the rollback the \nfirst replicaset had 0 PODs and the new replicaset had 5 PODs and this is reversed \nafter the rollback is finished.\n80MUMSHAD MANNAMBETH\nkubectl run\n> kubectl run nginx --image=nginx\ndeployment "nginx" created\nAnd finally let’s get back to one of the commands we ran initially when we learned \nabout PODs for the first time. We used the kubectl run command to create a POD. \nThis command infact creates a deployment and not just a POD. This is why the output \nof the command says Deployment nginx created. This is another way of creating a \ndeployment by only specifying the image name and not using a definition file. A \nreplicaset and pods are automatically created in the backend. Using a definition file is \nrecommended though as you can save the file, check it into the code repository and \nmodify it later as required.\n81MUMSHAD MANNAMBETH\nSummarize Commands\n> kubectl rollout status deployment/myapp-deployment\n> kubectl rollout history deployment/myapp-deployment\n> kubectl create –f deployment-definition.yml\n> kubectl get deployments\n> kubectl apply –f deployment-definition.yml\n> kubectl set image deployment/myapp-deployment nginx=nginx:1.9.1\n> kubectl rollout undo deployment/myapp-deployment\nCreate\nGet\nUpdate\nStatus\nRollback\nTo summarize the commands real quick, use the kubectl create command to create \nthe deployment, get deployments command to list the deployments, apply and set \nimage commands to update the deployments, rollout status command to see the \nstatus of rollouts and rollout undo command to rollback a deployment operation.\n82MUMSHAD MANNAMBETH\nDemo\nDeployment\nThat’s it for this lecture. We will now head over to a Demo and I will see you in the \nnext lecture.\n83MUMSHAD MANNAMBETH\nNetworking 101\nmumshad mannambeth\nIn this lecture we will discuss about Networking in kubernetes.\n84MUMSHAD MANNAMBETH\nNode\n10.244.0.0\nMy System\nKubernetes Networking - 101\nPOD\n192.168.1.2\nMinikube\n Node\n192.168.1.2\n10.244.0.2\nPOD\n10.244.0.4\nPOD\n10.244.0.3\n• IP Address is assigned to a POD\n192.168.1.10\nLet us look at the very basics of networking in Kubernetes. We will start with a single \nnode kubernetes cluster. The node has an IP address, say it is 192.168.1.2 in this case. \nThis is the IP address we use to access the kubernetes node, SSH into it etc. On a side \nnote, remember if you are using a MiniKube setup, then I am talking about the IP \naddress of the minikube virtual machine inside your Hypervisor. Your laptop may be \nhaving a different IP like 192.168.1.10. So its important to understand how VMs are \nsetup. \nSo on the single node kubernetes cluster we have created a Single POD.  As you know \na POD hosts a container.  Unlike in the docker world were an IP address is always \nassigned to a Docker CONTAINER, \nin Kubernetes the IP address is assigned to a POD. Each POD in kubernetes gets its \nown internal IP Address. In this case its in the range 10.244 series and the IP assigned \nto the POD is 10.244.0.2. So how is it getting this IP address? When Kubernetes is \ninitially configured it creates an internal private network with the address 10.244.0.0 \nand all PODs are attached to it. When you deploy multiple PODs, they all get a \nseparate IP assigned. The PODs can communicate to each other through this IP . But \naccessing other PODs using this internal IP address MAY not be a good idea as its \nsubject to change when PODs are recreated. We will see BETTER ways to establish \n85communication between PODs in a while. For now its important to understand how \nthe internal networking works in kubernetes.\n85MUMSHAD MANNAMBETH\nCluster Networking\nNode\n192.168.1.2\nNode\n192.168.1.3\n10.244.0.0\nPOD\n10.244.0.2\nPOD\n10.244.0.2\n10.244.0.0\nKubernetes Cluster\n• All containers/PODs can communicate to one \nanother without NAT\n• All nodes can communicate with all \ncontainers and vice-versa without NAT\nSo it’s all easy and simple to understand when it comes to networking on a single \nnode. But how does it work when you have multiple nodes in a cluster? In this case \nwe have two nodes running kubernetes and they have IP addresses 192.168.1.2 and \n192.168.1.3 assigned to them. Note that they are not part of the same cluster yet. \nEach of them has a single POD deployed. As discussed in the previous slide these \npods are attached to an internal network and they have their own IP addresses \nassigned. HOWEVER, if you look at the network addresses, you can see that they are \nthe same. The two networks have an address 10.244.0.0 and the PODs deployed have \nthe same address too. \nThis is NOT going to work well when the nodes are part of the same cluster. The \nPODs have the same IP addresses assigned to them and that will lead to IP conflicts in \nthe network. Now that’s ONE problem. When a kubernetes cluster is SETUP , \nkubernetes does NOT automatically setup any kind of networking to handle these \nissues. As a matter of fact, kubernetes expects US to setup networking to meet \ncertain fundamental requirements. Some of these are that all the containers or PODs \nin a kubernetes cluster MUST be able to communicate with one another without \nhaving to configure NAT. All nodes must be able to communicate with containers and \nall containers must be able to communicate with the nodes in the cluster. Kubernetes \n86expects US to setup a networking solution that meets these criteria.  \n86MUMSHAD MANNAMBETH\nFortunately, we don’t have to set it up ALL on our own as there are multiple pre-built \nsolutions available. Some of them are the cisco ACI networks, Cilium, Big Cloud \nFabric, Flannel, Vmware NSX-t and Calico. Depending on the platform you are \ndeploying your Kubernetes cluster on you may use any of these solutions. For \nexample, if you were setting up a kubernetes cluster from scratch on your own \nsystems, you may use any of these solutions like Calico, Flannel etc. If you were \ndeploying on a Vmware environment NSX-T may be a good option. If you look at the \nplay-with-k8s labs they use WeaveNet. In our demos in the course we used Calico. \nDepending on your environment and after evaluating the Pros and Cons of each of \nthese, you may chose the right networking solution.\n87MUMSHAD MANNAMBETH\nCluster Networking Setup\nIf you go back and look at the demo were we setup a kubernetes cluster initially, we \nsetup networking based on Calico. It only took us a few set of commands to get it \nsetup. So its pretty easy.\n88MUMSHAD MANNAMBETH\nCluster Networking\nNode\n192.168.1.2\nNode\n192.168.1.3\n10.244.1.0\nPOD\n10.244.1.2\nPOD\n10.244.2.2\n10.244.2.0\nKubernetes Cluster\nRouting\nSo back to our cluster, with the Calico networking setup, it now manages the \nnetworks and Ips in my nodes and assigns a different network address for each \nnetwork in the nodes. This creates a virtual network of all PODs and nodes were they \nare all assigned a unique IP Address. And by using simple routing techniques the \ncluster networking enables communication between the different PODs or Nodes to \nmeet the networking requirements of kubernetes. Thus all PODs can now \ncommunicate to each other using the assigned IP addresses.\n89MUMSHAD MANNAMBETH\nDemo\nNetworking\nThat’s it for this lecture. We will now head over to a Demo and I will see you in the \nnext lecture.\n90MUMSHAD MANNAMBETH\nServices\nmumshad mannambeth\nIn this lecture we will discuss about Kubernetes Services.\n91MUMSHAD MANNAMBETH\nServices\nServices\nServices\nKubernetes Services enable communication between various components within and \noutside of the application. Kubernetes Services helps us connect applications \ntogether with other applications or users. For example, our application has groups of \nPODs running various sections, such as a group for serving front-end load to users, \nanother group running back-end processes, and a third group connecting to an \nexternal data source. It is  Services that enable connectivity between these groups of \nPODs. Services enable the front-end application to be made available to users, it \nhelps communication between back-end and front-end PODs, and helps in \nestablishing connectivity to an external data source. Thus services enable loose \ncoupling between microservices in our application.\n92MUMSHAD MANNAMBETH\nService\nNode\n192.168.1.2\n10.244.0.0\nPOD\n10.244.0.2\n192.168.1.10\nSSH\n>curl http://10.244.0.2\nHello World!\n>curl http://192.168.1.2\nHello World! ?Service\n30008\n:30008\nLet’s take a look at one use case of Services. So far we talked about how PODs \ncommunicate with each other through internal networking. Let’s look at some other \naspects of networking in this lecture. Let’s start with external communication. So we \ndeployed our POD having a web application running on it.  How do WE as an external \nuser access the web page?  First of all, lets look at the existing setup. The Kubernetes \nNode has an IP address and that is 192.168.1.2. My laptop is on the same network as \nwell, so it has an IP address 192.168.1.10. The internal POD network is in the range \n10.244.0.0 and the POD has an IP 10.244.0.2. Clearly, I cannot ping or access the POD \nat address 10.244.0.2 as its in a separate network. So what are the options to see the \nwebpage?\nFirst, if we were to SSH into the kubernetes node at 192.168.1.2, from the node, we \nwould be able to access the POD’s webpage by doing a curl or if the node has a GUI, \nwe could fire up a browser and see the webpage in a browser following the address \nhttp://10.244.0.2.  But this is from inside the kubernetes Node and that’s not what I \nreally want. I want to be able to access the web server from my own laptop without \nhaving to SSH into the node and simply by accessing the IP of the kubernetes node. \nSo we need something in the middle to help us map requests to the node from our \nlaptop through the node to the POD running the web container. \n93That is where the kubernetes service comes into play. The kubernetes service is an \nobject just like PODs, Replicaset or Deployments that we worked with before. One of \nits use case is to listen to a port on the Node and forward requests on that port to a \nport on the POD running the web application.  This type of service is known as a \nNodePort service because the service listens to a port on the Node and forwards \nrequests to PODs. There are other kinds of services available which we will now \ndiscuss.\n93MUMSHAD MANNAMBETH\nServices Types\nNodePort ClusterIP LoadBalancer\nThe first one is what we discussed already – NodePort were the service makes an \ninternal POD accessible on a Port on the Node. The second is ClusterIP – and in this \ncase the service creates a virtual IP inside the cluster to enable communication \nbetween different services such as a set of front-end servers to a set of backend-\nservers.  The third type is a LoadBalancer, were it provisions a load balancer for our \nservice in supported cloud providers. A good example of that would be to distribute \nload across different web servers. We will now look at Each of these in a bit more \ndetail along with some Demos.\n94MUMSHAD MANNAMBETH\nNodePort\nIn this lecture we will discuss about the NodePort Kubernetes Service.\n95MUMSHAD MANNAMBETH\nService - NodePort\nNode\n192.168.1.2\n10.244.0.0\nPOD\n10.244.0.2\n192.168.1.10\n>curl http://192.168.1.2:30008\nHello World! ?Service\n30008\nGetting back to NodePort, few slides back we discussed about external access to the \napplication. We said that a Service can help us by mapping a port on the Node to a \nport on the POD.  \n96MUMSHAD MANNAMBETH\nService - NodePort\nNode\nPOD\n10.244.0.2\nService\n30008\n80\n80\nTargetPort1Port2\nNodePort3\n10.106.1.12Range: \n30000 - 32767\nLet’s take a closer look at the Service. If you look at it, there are 3 ports involved. The \nport on the POD were the actual web server is running is port 80. And it is referred to \nas the targetPort, because that is were the service forwards the requests to. The \nsecond port is the port on the service itself. It is simply referred to as the port. \nRemember, these terms are from the viewpoint of the service. The service is in fact \nlike a virtual server inside the node. Inside the cluster it has its own IP address.  And \nthat IP address is called the Cluster-IP of the service. And finally we have the port on \nthe Node itself which we use to access the web server externally. And that is known \nas the NodePort.  As you can see it is 30008. That is because NodePorts can only be in \na valid range which is from 30000 to 32767.\n97MUMSHAD MANNAMBETH\nService - NodePort\nNode\nPOD\n10.244.0.2\nService\n30008\n80\n80\nTargetPort1Port2\nNodePort3\n10.106.1.12Range: \n30000 - 32767\napiVersion: \nkind:\nmetadata:\nspec:\nservice-definition.yml\nv1\nService\nname: myapp-service\ntype: NodePort\nports:\n- targetPort: 80\nport: 80\nnodePort: 30008\n*\nLet us now look at how to create the service. Just like how we created a Deployment, \nReplicaSet or Pod, we will use a definition file to create a service. \nThe high level structure of the file remains the same. As before we have apiVersion, \nkind, metadata and spec sections. The apiVersion is going to be v1. The kind is \nofcourse service. The metadata will have a name and that will be the name of the \nservice. It can have labels, but we don’t need that for now. Next we have spec. and as \nalways this is the most crucial part of the file as this is were we will be defining the \nactual services and this is the part of a definition file that differs between different \nobjects. In the spec section of a service we have type and ports. The type refers to \nthe type of service we are creating. As discussed before it could be ClusterIP, \nNodePort, or LoadBalancer. In this case since we are creating a NodePort we will set \nit as NodePort. The next part of spec is ports. This is were we input information \nregarding what we discussed on the left side of this screen. The first type of port is \nthe targetPort, which we will set to 80. The next one is simply port, which is the port \non the service object and we will set that to 80 as well. The third is NodePort which \nwe will set to 30008 or any number in the valid range. Remember that out of these, \nthe only mandatory field is port . If you don’t provide a targetPort it is assumed to be \nthe same as port and if you don’t provide a nodePort a free port in the valid range \nbetween 30000 and 32767 is automatically allocated. Also note that ports is an array. \n98So note the dash under the ports section that indicate the first element in the array. \nYou can have multiple such port mappings within a single service.\nSo we have all the information in, but something is really missing. There is nothing \nhere in the definition file that connects the service to the POD. We have simply \nspecified the targetPort but we didn’t mention the targetPort on which POD. There \ncould be 100s of other PODs with web services running on port 80. So how do we do \nthat?\nAs we did with the replicasets previously and a technique that you will see very often \nin kubernetes, we will use labels and selectors to link these together.  We know that \nthe POD was created with a label. We need to bring that label into this service \ndefinition file. \n98MUMSHAD MANNAMBETH\nService - NodePort\napiVersion: \nkind:\nmetadata:\nspec:\nservice-definition.yml\nv1\nService\nname: myapp-service\ntype: NodePort\nports:\n- targetPort: 80\nport: 80\nnodePort: 30008\napiVersion: \nkind:\nmetadata:\nname: myapp-pod\nlabels:\nspec:\ncontainers:\n- name: nginx-container\nimage: nginx\npod-definition.yml\nv1\nPod\napp: myapp\ntype: front-end\nselector:\n> kubectl create –f service-definition.yml\nservice "myapp-service" created\n> kubectl get services\nNAME            TYPE        CLUSTER-IP       EXTERNAL-IP   PORT(S)        AGE\nkubernetes      ClusterIP 10.96.0.1        <none>        443/TCP        16d\nmyapp-service   NodePort 10.106.127.123   <none>        80:30008/TCP   5m\n> curl http://192.168.1.2:30008\nSo we have a new property in the spec section and that is selector. Under the \nselector provide a list of labels to identify the POD. For this refer to the pod-\ndefinition file used to create the POD. Pull the labels from the pod-definition file and \nplace it under the selector section. This links the service to the pod. Once done \ncreate the service using the kubectl create command and input the service-definition \nfile and there you have the service created.\nTo see the created service, run the kubectl get services command that lists the \nservices, their cluster-ip and the mapped ports. The type is NodePort as we created \nand the port on the node automatically assigned is 32432. We can now use this port \nto access the web service using curl or a web browser. \n99MUMSHAD MANNAMBETH\nService - NodePort\nNode\n192.168.1.2\nPOD\n10.244.0.2\nService\n30008\nPOD\n10.244.0.3\nPOD\n10.244.0.4\nlabels:\napp: myapp\nselector:\napp: myapp\nAlgorithm: Random\nSessionAffinity: Yes\nSo far we talked about a service mapped to a single POD. But that’s not the case all \nthe time, what do you do when you have multiple PODs? In a production \nenvironment you have multiple instances of your web application running for high-\navailability and load balancing purposes. \nIn this case we have multiple similar PODs running our web application. They all have \nthe same labels with a key app set to value myapp. The same label is used as a \nselector during the creation of the service. So when the service is created, it looks for \nmatching PODs with the labels and finds 3 of them. The service then automatically \nselects all the 3 PODs as endpoints to forward the external requests coming from the \nuser. You don’t have to do any additional configuration to make this happen. And if \nyou are wondering what algorithm it uses to balance load, it uses a random \nalgorithm.  Thus the service acts as a built-in load balancer to distribute load across \ndifferent PODs. \n100MUMSHAD MANNAMBETH\nNode\nNode\nService - NodePort\nNode\n192.168.1.2\nPOD\n10.244.0.2\nService\n30008\nPOD\n10.244.0.3\nPOD\n10.244.0.4\n192.168.1.3\n 192.168.1.430008 30008\n>curl http://192.168.1.2:30008\n>curl http://192.168.1.3:30008\n>curl http://192.168.1.4:30008\nAnd finally, lets look at what happens when the PODs are distributed across multiple \nnodes. In this case we have the web application on PODs on separate nodes in the \ncluster. When we create a service , without us having to do ANY kind of additional \nconfiguration, kubernetes creates a service that spans across all the nodes in the \ncluster and maps the target port to the SAME NodePort on all the nodes in the \ncluster. This way you can access your application using the IP of any node in the \ncluster and using the same port number which in this case is 30008.\nTo summarize – in ANY case weather it be a single pod in a single node, multiple pods \non a single node, multiple pods on multiple nodes, the service is created exactly the \nsame without you having to do any additional steps during the service creation. \nWhen PODs are removed or added the service is automatically updated making it \nhighly flexible and adaptive. Once created you won’t typically have to make any \nadditional configuration changes.\n101MUMSHAD MANNAMBETH\nDemo\nService - NodePort\nThat’s it for this lecture, head over to the demo and I will see you in the next lecture.\n102MUMSHAD MANNAMBETH\nClusterIP\nmumshad mannambeth\nIn this lecture we will discuss about the Kubernetes Service - ClusterIP .\n103MUMSHAD MANNAMBETH\nClusterIP\nPOD\n10.244.0.2\nPOD\n10.244.0.3\nPOD\n10.244.0.4\nPOD\n10.244.0.5\nPOD\n10.244.0.6\nPOD\n10.244.0.7\nPOD\n10.244.0.8\nPOD\n10.244.0.9\nPOD\n10.244.0.10\n10.244.0.5\nback-end\nfront-end\nback-end\nredis\nredis\nA full stack web application typically has different kinds of PODs hosting different \nparts of an application. You may have a number of PODs running a front-end web \nserver,  another set of PODs running a backend server, a set of PODs running a key-\nvalue store like Redis, another set of PODs running a persistent database like MySQL \netc.  The web front-end servers need to connect to the backend-workers and the \nbackend-workers need to connect to database as well as the redis services. So what \nIS the right way to establish connectivity between these PODs?\nThe PODs all have an IP address assigned to them as we can see on the screen. But \nthese Ips as we know are not static, these PODs can go down anytime and new PODs \nare created all the time – and so you CANNOT rely on these IP addresses for internal \ncommunication within the application.  Also what if the first front-end POD at \n10.244.0.3 need to connect to a backend service? Which of the 3 would it go to and \nwho makes that decision?\nA kubernetes service can help us group these PODs together and provide a single \ninterface to access the PODs in a group. For example a service created for the \nbackend PODs will help group all the backend PODs together and provide a single \ninterface for other PODs to access this service. The requests are forwarded to one of \n104the PODs under the service randomly. Similarly, create additional services for Redis \nand allow the backend PODs to access the redis system through this service. This \nenables us to easily and effectively deploy a microservices based application on \nkubernetes cluster. Each layer can now scale or move as required without impacting \ncommunication between the various services. Each service gets an IP and name \nassigned to it inside the cluster and that is the name that should be used by other \nPODs to access the service. This type of service is known as ClusterIP. \n104MUMSHAD MANNAMBETH\napiVersion: \nkind:\nmetadata:\nspec:\nservice-definition.yml\nv1\nService\nname: back-end\ntype: ClusterIP\nports:\n- targetPort: 80\nport: 80\nselector:\napiVersion: \nkind:\nmetadata:\nname: myapp-pod\nlabels:\nspec:\ncontainers:\n- name: nginx-container\nimage: nginx\npod-definition.yml\nv1\nPod\napp: myapp\ntype: back-end\n> kubectl create –f service-definition.yml\nservice “back-end" created\n> kubectl get services\nNAME            TYPE        CLUSTER-IP       EXTERNAL-IP   PORT(S)        AGE\nkubernetes      ClusterIP 10.96.0.1        <none>        443/TCP        16d\nback-end        ClusterIP 10.106.127.123   <none>        80/TCP         2m\nTo create such a service, as always, use a definition file. In the service definition file , \nfirst use the default template which has apiVersion, kind, metadata and spec. The \napiVersion is v1 , kind is Service and we will give a name to our service – we will call \nit back-end. Under Specification we have type and ports. The type is ClusterIP. In \nfact, ClusterIP is the default type, so even if you didn’t specify it, it will automatically \nassume it to be ClusterIP. Under ports we have a targetPort and port. The target port \nis the port were the back-end is exposed, which in this case is 80. And the port is \nwere the service is exposed. Which is 80 as well. To link the service to a set of PODs, \nwe use selector. \nWe will refer to the pod-definition file and copy the labels from it and move it under \nselector.  And that should be it. We can now create the service using the kubectl\ncreate command and then check its status using the kubectl get services command. \nThe service can be accessed by other PODs using the ClusterIP or the service name.\n105MUMSHAD MANNAMBETH\nDemo\nService - NodePort\nThat’s it for this lecture, head over to the demo and I will see you in the next lecture.\n106MUMSHAD MANNAMBETH\nReferences\nhttps://kubernetes.io/docs/concepts/services-networking/dns-pod-service/\n107MUMSHAD MANNAMBETH\nService -\nLoadBalancer\nmumshad mannambeth\nIn this lecture we will discuss about the third kind of Kubernetes Service -\nLoadBalancer.\n108MUMSHAD MANNAMBETH\nNode\nNode\nServices\nNode\n192.168.1.2\nPOD\n10.244.1.3\nService\n30008\nPOD\n10.244.0.3\nPOD\n10.244.2.3\n192.168.1.3\n 192.168.1.430008 30008\n>curl http://192.168.1.2:30008\n>curl http://192.168.1.3:30008\n>curl http://192.168.1.4:30008\nPOD\n10.244.1.4\nPOD\n10.244.0.4\nPOD\n10.244.2.4\nService - ClusterIP\n- NodePort\n>curl http://myapp.com\nLoad Balancer\nNative Load Balancer\nWe will quickly recap what we learned about the two service types, so that we can \nwork our way to the LoadBalancer type.  We have a 3 node cluster with Ips\n192.168.1.2,3 and 4.  Our application is two tier, there is a database service and a \nfront-end web service for users to access the application. The default service type –\nknown as ClusterIP – makes a service, such as a redis or database service available \ninternally within the kubernetes cluster for other applications to consume.  \nThe next tier in my application happens to be a python based web front-end. This \napplication connects to the backend using Service created for the redis service. To \nexpose the application to the end users, we create another service of type NodePort. \nCreating a service of type NodePort exposes the application on a high end port of the \nNode and the users can access the application at any IP of my nodes with the port \n30008. \nNow, what IP do you give your end users to access your application? You cannot give \nthem all three and let them choose one of their own. What end users really want is a \nsingle URL to access the application. For this, you will be required to setup a separate \nLoad Balancer VM in your environment. In this case I deploy a new VM for load \nbalancer purposes and configure it to forward requests that come to it to any of the \n109Ips of the Kubernetes nodes. I will then configure my organizations DNS to point to \nthis load balancer when a user hosts http://myapp.com. Now setting up that load \nbalancer by myself is a tedious task, and I might have to do that in my local or on-\nprem environment. However, if I happen to be on a supported CloudPlatform, like \nGoogle Cloud Platform, I could leverage the native load balancing functionalities of \nthe cloud platform to set this up. Again you don’t have to set that up manually, \nKubernetes sets it up for you. Kubernetes has built-in integration with supported \ncloud platforms.\n109MUMSHAD MANNAMBETH\napiVersion: \nkind:\nmetadata:\nspec:\nservice-definition.yml\nv1\nService\nname: front-end\ntype:\nports:\n- targetPort: 80\nport: 80\nselector:\napp: myapp\ntype: front-end\nNodePortLoadBalancer\n> kubectl create –f service-definition.yml\nservice “front-end" created\n> kubectl get services\nNAME            TYPE        CLUSTER-IP       EXTERNAL-IP   PORT(S)        AGE\nkubernetes      ClusterIP 10.96.0.1        <none>        443/TCP        16d\nfront-end       LoaBalancer 10.106.127.123   <Pending>     80/TCP         2m\n110MUMSHAD MANNAMBETH\nMicroservices\nmumshad mannambeth\nIn this lecture we will try and understand Microservices architecture using a simple \nweb application. We will then try and deploy this web application on multiple \ndifferent kubernetes platforms.\n111MUMSHAD MANNAMBETH\nExample voting app\nPOD\nServicePOD\nService\nPOD\nPOD\nresult-app\nPOD\nvoting-app\nredis db\nworker\nService Service\nGoals:\n1. Deploy Containers\n2. Enable Connectivity\n3. External Access\nSteps:\n1. Deploy PODs\n2. Create Services (ClusterIP)\n3. Create Services (LoadBalancer)\nSo this is what we saw in the last demo. We deployed PODs and services to keep it \nreally simple. But this has its own challenges. Deploying PODs doesn’t help us scale \nour application easily. Also if a POD was to fail it doesn’t come back up or deploy a \nnew POD automatically. We could deploy multiple PODs one after the other, but \nthere are easier ways to scale using ReplicaSets and Deployments as we discussed in \nthe lectures.  \n112MUMSHAD MANNAMBETH\nDeployment\nDeploymentDeployment\nDeploymentDeployment\nExample voting app\nPODPOD\nPOD\nPOD\nresult-app\nPOD\nvoting-app\nredis db\nworker\nService Service\nSteps:\n1. Create Deployments\n2. Create Services (ClusterIP)\n3. Create Services (LoadBalancer)\nServiceService\nPOD\nvoting-app\nPOD\nvoting-app\nPOD\nresult-app\nPOD\nresult-app\nLet us now improvise our setup using Deployments. We chose deployments over \nReplicaSets as Deployments automatically create replica sets as required and it can \nhelp us perform rolling updates and roll backs etc. Deployments are the way to go. So \nwe add more PODs for the front end applications voting-app and result-app by \ncreating a deployment with 3 replicas. We also encapsulate database and workers in \ndeployments. Let’s take a look at that now.\n113MUMSHAD MANNAMBETH\nNode\nNode\nNode\nNode\nDeploymentDeployment\nExample voting app\nPOD\nresult-app\nPOD\nvoting-app\nService Service\nPOD\nvoting-app\nPOD\nvoting-app\nPOD\nresult-app\nPOD\nresult-app\n192.168.56.70 192.168.56.71 192.168.56.72 192.168.56.73\nhttp://192.168.56.70:30035\nhttp://192.168.56.71:30035\nhttp://192.168.56.72:30035\nhttp://192.168.56.73:30035\nhttp://example-vote.com\nhttp://192.168.56.70:31061\nhttp://192.168.56.71:31061\nhttp://192.168.56.72:31061\nhttp://192.168.56.73:31061\nhttp://example-result.com\nLoad Balancer\nLet us now focus on the front end applications – voting-app and result-app. We know \nthat these PODs are hosted on different Nodes. The services with type NodePOrt help \nin receiving traffic on the ports on the Nodes and routing and load balancing them to \nappropriate PODs. But what IP address would you give your end users to access the \napplications. You could access any of these two applications using IP of any of the \nNodes and the high end port the service is exposed on. That would be 4 IP and port \ncombination for the voting-app and 4 IP and port combination for the result-app. But \nthat’s not what the end users want. They need a single URL like example-vote.com or \nexample-result.com. One way to achieve this in my current VirtualBox setup is to \ncreate a new VM for Load Balancer purpose and install and configure a suitable load \nbalancer on it like HAProxy or NGINX etc. The load balancer would then re-route \ntraffic to underlying nodes and then through to PODs to serve the users.\n114MUMSHAD MANNAMBETH\nNode\nNode\nNode\nNode\nDeploymentDeployment\nExample voting app\nPOD\nresult-app\nPOD\nvoting-app\nService Service\nPOD\nvoting-app\nPOD\nvoting-app\nPOD\nresult-app\nPOD\nresult-app\n192.168.56.70 192.168.56.71 192.168.56.72 192.168.56.73\nhttp://example-vote.com\nhttp://example-result.com\nLoad Balancer\nNative Load Balancer\nNow setting all of that external load balancing can be a tedious task. However, if I was \non a supported cloud platform like Google Cloud, I could leverage the native load \nbalancer to do that configuration for me. Kubernetes has support for getting that \ndone for us automatically. All you need to do is set the Service Type for the front end \nservices to LoadBalancer. Remember this only, works with supported cloud platforms. \nIf you set the Type of service to LoadBalancer in an unsupported environment like \nVirtualBox, then it would simply be considered as if you set it to NodePOrt, were the \nservices are exported on a high-end port on the Nodes. The same setup we had \nearlier.  Let us take a look at how we can achieve this on Google Cloud Platform.\n115MUMSHAD MANNAMBETH\nConclusion\nKubernetes Overview\nContainers – Docker\nContainer Orchestration?\nDemo - Setup Kubernetes\nKubernetes Concepts – PODs | ReplicaSets | Deployment | Services\nNetworking in Kubernetes\nKubernetes Management - Kubectl\nKubernetes Definition Files - YAML\nKubernetes on Cloud – AWS/GCP\nWe are at the end of the Kubernetes for Beginners course. I hope I have covered \nenough topics to get you started with Kubernetes. We started with Containers and \nDocker and we looked what container orchestration is. We looked at various options \navailable to setup kubernetes. We went through some of the concepts such as PODs, \nReplicaSets, Deployments and Services. We also looked at Networking in Kubernetes. \nWe also spent some time on working with kubectl commands and kubernetes \ndefinition files. I hope you got enough hands-on experience in developing kubernetes \ndefinition files. And finally we also saw how to deploy a sample microservices \napplication on Kubernetes on Google Cloud Platform. \nI will be adding additional topics to the course in the near future. So watch out for an \nannouncement from me. In case you need me to cover any other topics, feel free to \nsend me a message or post a question and I will try my best to add a new lecture on \nit. \nWe also have an assignment as part of this course. So if you have time, please go \nahead and develop your solution and submit your assignment. Feel free to review \nother students assignments and share your views on them.\n116It was really nice having you for this course. I hope you gained enough knowledge and \nexperience to get started with kubernetes at your work or otherwise and I wish you \ngood luck in your Kubernetes Journey. See you in the next course, until then Good \nBye!\n116']
2025-12-19 17:24:26,466 - INFO - Text extracted from pdf and converted into chunks
2025-12-19 17:24:26,477 - ERROR - Failed to perform db operations Collection [Kubernetes-for-Beginners.pdf] already exists
2025-12-19 17:24:51,230 - INFO - db configuration is localhost, 8000
2025-12-19 17:24:51,440 - INFO - successfully connected to chromadb
2025-12-19 17:25:01,724 - INFO - Text extracted from pdf and converted into chunks
2025-12-19 17:25:01,728 - ERROR - Failed to perform db operations Collection [Kubernetes-for-Beginners.pdf] already exists
2025-12-19 17:25:29,687 - INFO - db configuration is localhost, 8000
2025-12-19 17:25:29,925 - INFO - successfully connected to chromadb
2025-12-19 17:25:46,882 - INFO - db configuration is localhost, 8000
2025-12-19 17:25:47,085 - INFO - successfully connected to chromadb
2025-12-19 17:29:10,452 - INFO - db configuration is localhost, 8000
2025-12-19 17:29:10,647 - INFO - successfully connected to chromadb
2025-12-19 17:31:10,284 - INFO - db configuration is localhost, 8000
2025-12-19 17:31:10,488 - INFO - successfully connected to chromadb
2025-12-19 17:31:19,835 - INFO - Text extracted from pdf and converted into chunks
2025-12-19 17:31:19,840 - ERROR - Failed to perform db operations Collection [Kubernetes-for-Beginners.pdf] already exists
2025-12-19 17:31:56,385 - INFO - Text extracted from pdf and converted into chunks
2025-12-19 17:31:56,389 - ERROR - Failed to perform db operations Collection [Kubernetes-for-Beginners.pdf] already exists
2025-12-19 17:32:25,340 - INFO - db configuration is localhost, 8000
2025-12-19 17:32:25,541 - INFO - successfully connected to chromadb
2025-12-19 17:32:35,467 - INFO - Text extracted from pdf and converted into chunks
2025-12-19 17:32:35,626 - ERROR - Failed to perform db operations 401 API keys are not supported by this API. Expected OAuth2 access token or other authentication credentials that assert a principal. See https://cloud.google.com/docs/authentication [reason: "CREDENTIALS_MISSING"
domain: "googleapis.com"
metadata {
  key: "service"
  value: "generativelanguage.googleapis.com"
}
metadata {
  key: "method"
  value: "google.ai.generativelanguage.v1beta.GenerativeService.BatchEmbedContents"
}
]
2025-12-19 17:37:06,999 - INFO - db configuration is localhost, 8000
2025-12-19 17:37:07,212 - INFO - successfully connected to chromadb
2025-12-19 17:37:48,151 - INFO - Text extracted from pdf and converted into chunks
2025-12-19 17:37:48,302 - ERROR - Failed to perform db operations 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. To monitor your current usage, head to: https://ai.dev/usage?tab=rate-limit. 
* Quota exceeded for metric: generativelanguage.googleapis.com/embed_content_free_tier_requests, limit: 0, model: embedding-001
* Quota exceeded for metric: generativelanguage.googleapis.com/embed_content_free_tier_requests, limit: 0, model: embedding-001
* Quota exceeded for metric: generativelanguage.googleapis.com/embed_content_free_tier_requests, limit: 0, model: embedding-001
* Quota exceeded for metric: generativelanguage.googleapis.com/embed_content_free_tier_requests, limit: 0, model: embedding-001
Please retry in 11.704715181s. [links {
  description: "Learn more about Gemini API quotas"
  url: "https://ai.google.dev/gemini-api/docs/rate-limits"
}
, violations {
  quota_metric: "generativelanguage.googleapis.com/embed_content_free_tier_requests"
  quota_id: "EmbedContentRequestsPerDayPerProjectPerModel-FreeTier"
  quota_dimensions {
    key: "model"
    value: "embedding-001"
  }
  quota_dimensions {
    key: "location"
    value: "global"
  }
}
violations {
  quota_metric: "generativelanguage.googleapis.com/embed_content_free_tier_requests"
  quota_id: "EmbedContentRequestsPerMinutePerProjectPerModel-FreeTier"
  quota_dimensions {
    key: "model"
    value: "embedding-001"
  }
  quota_dimensions {
    key: "location"
    value: "global"
  }
}
violations {
  quota_metric: "generativelanguage.googleapis.com/embed_content_free_tier_requests"
  quota_id: "EmbedContentRequestsPerMinutePerUserPerProjectPerModel-FreeTier"
  quota_dimensions {
    key: "model"
    value: "embedding-001"
  }
  quota_dimensions {
    key: "location"
    value: "global"
  }
}
violations {
  quota_metric: "generativelanguage.googleapis.com/embed_content_free_tier_requests"
  quota_id: "EmbedContentRequestsPerDayPerUserPerProjectPerModel-FreeTier"
  quota_dimensions {
    key: "model"
    value: "embedding-001"
  }
  quota_dimensions {
    key: "location"
    value: "global"
  }
}
, retry_delay {
  seconds: 11
}
]
2025-12-19 17:39:20,959 - INFO - Text extracted from pdf and converted into chunks
2025-12-19 17:39:21,582 - INFO - Kubernetes-for-Beginners.pdf
2025-12-19 17:39:21,587 - INFO - Collection(name=Kubernetes-for-Beginners.pdf)
2025-12-19 17:39:21,902 - ERROR - Failed to generate answer 404 models/gemini-pro is not found for API version v1beta, or is not supported for generateContent. Call ListModels to see the list of available models and their supported methods.
2025-12-19 17:39:21,902 - ERROR - Failed to make rag prompt 404 models/gemini-pro is not found for API version v1beta, or is not supported for generateContent. Call ListModels to see the list of available models and their supported methods.
2025-12-19 17:39:21,902 - ERROR - Failed to get relevent passage, Please change the query
2025-12-19 17:39:21,902 - ERROR - Failed to perform db operations 404 models/gemini-pro is not found for API version v1beta, or is not supported for generateContent. Call ListModels to see the list of available models and their supported methods.
2025-12-19 17:40:40,867 - INFO - db configuration is localhost, 8000
2025-12-19 17:40:41,064 - INFO - successfully connected to chromadb
2025-12-19 17:41:12,289 - INFO - Text extracted from pdf and converted into chunks
2025-12-19 17:41:12,628 - INFO - Kubernetes-for-Beginners.pdf
2025-12-19 17:41:12,632 - INFO - Collection(name=Kubernetes-for-Beginners.pdf)
